{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ml4cM7hUR-wJ",
        "EsqbL4myTVqe",
        "kDpwh6g0Smx7",
        "dsGNaS0lSZhl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl_Ax4zmR-Vj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where Airflow, Spark, Snowflake & Databricks Fit in Document Processing\n",
        "\n",
        "These enterprise-grade platforms complement LlamaIndex pipeline by handling **orchestration**, **scale**, **storage**, and **governance**. Here's where each fits in the document processing architecture:"
      ],
      "metadata": {
        "id": "oeWQX538TpG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ðŸ”„ **Apache Airflow - Workflow Orchestration**\n",
        "\n",
        "**Role**: End-to-end pipeline orchestration and scheduling[1][2][3]\n",
        "\n",
        "**Key Benefits**:\n",
        "- **Dependency Management**: Ensures tasks run in correct order[2][4]\n",
        "- **Error Handling**: Automatic retries and failure notifications[3]\n",
        "- **Monitoring**: Web UI for pipeline visibility and debugging[5]\n",
        "- **Scheduling**: Cron-based or event-driven execution[6]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ml4cM7hUR-wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def document_ingestion_dag():\n",
        "    \"\"\"Complete document processing pipeline with Airflow\"\"\"\n",
        "\n",
        "    dag = DAG(\n",
        "        'document_processing_pipeline',\n",
        "        default_args={\n",
        "            'owner': 'data-team',\n",
        "            'depends_on_past': False,\n",
        "            'start_date': datetime(2025, 1, 1),\n",
        "            'email_retries': 2,\n",
        "            'retry_delay': timedelta(minutes=5)\n",
        "        },\n",
        "        description='End-to-end document processing with LlamaIndex',\n",
        "        schedule_interval='@daily',  # Run daily\n",
        "        catchup=False,\n",
        "        max_active_runs=1\n",
        "    )\n",
        "\n",
        "    # Task 1: Extract documents from various sources\n",
        "    extract_documents = PythonOperator(\n",
        "        task_id='extract_documents',\n",
        "        python_callable=extract_from_sources,\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Task 2: Process with LlamaIndex + Spark for large datasets\n",
        "    process_documents = PythonOperator(\n",
        "        task_id='process_with_llamaindex_spark',\n",
        "        python_callable=run_spark_llamaindex_pipeline,\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Task 3: Store embeddings in Snowflake\n",
        "    store_embeddings = PythonOperator(\n",
        "        task_id='store_in_snowflake',\n",
        "        python_callable=store_vectors_snowflake,\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Task 4: Update vector index\n",
        "    update_vector_index = PythonOperator(\n",
        "        task_id='update_vector_index',\n",
        "        python_callable=refresh_pinecone_index,\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Task dependencies\n",
        "    extract_documents >> process_documents >> store_embeddings >> update_vector_index\n",
        "\n",
        "    return dag\n",
        "\n"
      ],
      "metadata": {
        "id": "eeKp2fjMThHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## âš¡ **Apache Spark - Large-Scale Processing**\n",
        "\n",
        "**Role**: Distributed processing for massive document datasets[7][8][9]\n",
        "\n",
        "**Key Benefits**:\n",
        "- **Horizontal Scaling**: Process TB+ of documents across clusters[10]\n",
        "- **In-Memory Processing**: 100x faster than traditional ETL[7]\n",
        "- **Fault Tolerance**: Automatic recovery from node failures[11]\n",
        "- **Integration**: Works with Hadoop, S3, databases, streaming sources[8]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EsqbL4myTVqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "import spark_nlp\n",
        "from llamaindex_spark_integration import SparkLlamaIndexProcessor\n",
        "\n",
        "def spark_document_processing():\n",
        "    \"\"\"Process millions of documents using Spark + LlamaIndex\"\"\"\n",
        "\n",
        "    # Initialize Spark with NLP capabilities\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"LlamaIndex-Spark-Pipeline\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Load documents from data lake (S3, HDFS, etc.)\n",
        "    documents_df = spark.read \\\n",
        "        .format(\"parquet\") \\\n",
        "        .load(\"s3a://company-datalake/documents/\") \\\n",
        "        .filter(col(\"document_type\").isin(\"pdf\", \"docx\", \"txt\"))\n",
        "\n",
        "    # Spark NLP preprocessing\n",
        "    from sparknlp.annotator import DocumentAssembler, Tokenizer, Normalizer\n",
        "    from sparknlp.base import Pipeline\n",
        "\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(\"content\") \\\n",
        "        .setOutputCol(\"document\")\n",
        "\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([\"document\"]) \\\n",
        "        .setOutputCol(\"tokens\")\n",
        "\n",
        "    normalizer = Normalizer() \\\n",
        "        .setInputCols([\"tokens\"]) \\\n",
        "        .setOutputCol(\"normalized\") \\\n",
        "        .setLowercase(True)\n",
        "\n",
        "    # Create Spark NLP pipeline\n",
        "    spark_nlp_pipeline = Pipeline(stages=[\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        normalizer\n",
        "    ])\n",
        "\n",
        "    # Process documents in parallel across cluster\n",
        "    processed_df = spark_nlp_pipeline.fit(documents_df).transform(documents_df)\n",
        "\n",
        "    # Custom UDF to integrate with LlamaIndex\n",
        "    def llamaindex_chunk_and_embed(content):\n",
        "        \"\"\"Process individual documents with LlamaIndex\"\"\"\n",
        "        from llama_index.core import Document\n",
        "        from llama_index.core.node_parser import SentenceSplitter\n",
        "        from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "        # Create LlamaIndex document\n",
        "        doc = Document(text=content)\n",
        "\n",
        "        # Chunk with LlamaIndex\n",
        "        splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "        nodes = splitter.get_nodes_from_documents([doc])\n",
        "\n",
        "        # Generate embeddings\n",
        "        embed_model = OpenAIEmbedding()\n",
        "        for node in nodes:\n",
        "            node.embedding = embed_model.get_text_embedding(node.text)\n",
        "\n",
        "        return [{\"text\": node.text, \"embedding\": node.embedding, \"metadata\": node.metadata}\n",
        "                for node in nodes]\n",
        "\n",
        "    # Apply LlamaIndex processing across partitions\n",
        "    from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
        "\n",
        "    llamaindex_udf = udf(llamaindex_chunk_and_embed, ArrayType(StructType([\n",
        "        StructField(\"text\", StringType()),\n",
        "        StructField(\"embedding\", ArrayType(FloatType())),\n",
        "        StructField(\"metadata\", StringType())\n",
        "    ])))\n",
        "\n",
        "    # Process and flatten results\n",
        "    final_df = processed_df \\\n",
        "        .withColumn(\"chunks\", llamaindex_udf(col(\"normalized.result\"))) \\\n",
        "        .select(explode(col(\"chunks\")).alias(\"chunk\")) \\\n",
        "        .select(\n",
        "            col(\"chunk.text\"),\n",
        "            col(\"chunk.embedding\"),\n",
        "            col(\"chunk.metadata\")\n",
        "        )\n",
        "\n",
        "    # Write to data warehouse\n",
        "    final_df.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"mergeSchema\", \"true\") \\\n",
        "        .save(\"s3a://company-datalake/processed-embeddings/\")\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "b5V1coODTZMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”ï¸ **Snowflake - Data Warehouse & RAG Storage**\n",
        "\n",
        "**Role**: Centralized storage, governance, and native RAG capabilities[12][13][14]\n",
        "\n",
        "**Key Benefits**:\n",
        "- **Native Vector Support**: Built-in VECTOR data type and similarity functions[14]\n",
        "- **Cortex AI**: Serverless LLM and embedding services[15][16]\n",
        "- **Zero-ETL**: Direct integration with data lakes and warehouses[13]\n",
        "- **Governance**: Row-level security, data masking, audit trails[17]"
      ],
      "metadata": {
        "id": "kDpwh6g0Smx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import snowflake.connector\n",
        "from llama_index.vector_stores.snowflake import SnowflakeVectorStore\n",
        "\n",
        "def snowflake_rag_integration():\n",
        "    \"\"\"Store and query document embeddings in Snowflake\"\"\"\n",
        "\n",
        "    # Connect to Snowflake\n",
        "    conn = snowflake.connector.connect(\n",
        "        user='your_user',\n",
        "        password='your_password',\n",
        "        account='your_account',\n",
        "        warehouse='COMPUTE_WH',\n",
        "        database='DOCUMENT_DB',\n",
        "        schema='RAG_SCHEMA'\n",
        "    )\n",
        "\n",
        "    # Create tables for document storage and embeddings\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Document metadata table\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS DOCUMENT_METADATA (\n",
        "            document_id STRING PRIMARY KEY,\n",
        "            source_path STRING,\n",
        "            document_type STRING,\n",
        "            processed_date TIMESTAMP_NTZ,\n",
        "            metadata VARIANT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Embeddings table with Snowflake's VECTOR type\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS DOCUMENT_EMBEDDINGS (\n",
        "            chunk_id STRING PRIMARY KEY,\n",
        "            document_id STRING,\n",
        "            chunk_text STRING,\n",
        "            embedding VECTOR(FLOAT, 1536),  -- OpenAI embedding dimension\n",
        "            chunk_metadata VARIANT,\n",
        "            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Use Snowflake Cortex for native RAG\n",
        "    def create_cortex_search_service():\n",
        "        \"\"\"Create Snowflake Cortex Search service\"\"\"\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE OR REPLACE CORTEX SEARCH SERVICE DOCUMENT_SEARCH\n",
        "            ON chunk_text\n",
        "            WAREHOUSE = COMPUTE_WH\n",
        "            TARGET_LAG = '1 minute'\n",
        "            AS (\n",
        "                SELECT chunk_id, chunk_text, chunk_metadata, embedding\n",
        "                FROM DOCUMENT_EMBEDDINGS\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "    # Query using Snowflake's native vector similarity\n",
        "    def vector_search(query_text, limit=5):\n",
        "        \"\"\"Search documents using Snowflake vector similarity\"\"\"\n",
        "\n",
        "        # Generate query embedding (using Snowflake Cortex)\n",
        "        query_embedding_sql = f\"\"\"\n",
        "            SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', '{query_text}') as query_embedding\n",
        "        \"\"\"\n",
        "\n",
        "        cursor.execute(query_embedding_sql)\n",
        "        query_embedding = cursor.fetchone()\n",
        "\n",
        "        # Vector similarity search\n",
        "        similarity_search = f\"\"\"\n",
        "            SELECT\n",
        "                chunk_text,\n",
        "                chunk_metadata,\n",
        "                VECTOR_COSINE_SIMILARITY(embedding, PARSE_JSON('{query_embedding}')) as similarity_score\n",
        "            FROM DOCUMENT_EMBEDDINGS\n",
        "            ORDER BY similarity_score DESC\n",
        "            LIMIT {limit}\n",
        "        \"\"\"\n",
        "\n",
        "        cursor.execute(similarity_search)\n",
        "        return cursor.fetchall()\n",
        "\n",
        "    # Use Snowflake Cortex for complete RAG\n",
        "    def snowflake_rag_query(question):\n",
        "        \"\"\"Complete RAG using Snowflake Cortex\"\"\"\n",
        "\n",
        "        rag_sql = f\"\"\"\n",
        "            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "                'mistral-7b',\n",
        "                CONCAT(\n",
        "                    'Context: ',\n",
        "                    (SELECT LISTAGG(chunk_text, '\\\\n')\n",
        "                     FROM (\n",
        "                         SELECT chunk_text,\n",
        "                         VECTOR_COSINE_SIMILARITY(\n",
        "                             embedding,\n",
        "                             SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', '{question}')\n",
        "                         ) as similarity\n",
        "                         FROM DOCUMENT_EMBEDDINGS\n",
        "                         ORDER BY similarity DESC\n",
        "                         LIMIT 3\n",
        "                     )\n",
        "                    ),\n",
        "                    '\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:'\n",
        "                )\n",
        "            ) as response\n",
        "        \"\"\"\n",
        "\n",
        "        cursor.execute(rag_sql)\n",
        "        return cursor.fetchone()\n",
        "\n",
        "    return {\n",
        "        'search': vector_search,\n",
        "        'rag_query': snowflake_rag_query\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "YAsn4jbRSnTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§± **Databricks - Unified AI Platform**\n",
        "\n",
        "**Role**: End-to-end MLOps/LLMOps with integrated vector search[18][19][20]\n",
        "\n",
        "**Key Benefits**:\n",
        "- **Unity Catalog**: Centralized governance for data, models, and vectors[21][22]\n",
        "- **Auto-Sync Indexes**: Vector indexes automatically update with source data[23][24]\n",
        "- **MLOps Integration**: End-to-end model lifecycle with MLflow[20][18]\n",
        "- **Foundation Models**: Pre-trained LLMs and embedding models[25]"
      ],
      "metadata": {
        "id": "dsGNaS0lSZhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import databricks\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from llama_index.vector_stores.databricks import DatabricksVectorSearch\n",
        "\n",
        "def databricks_unified_rag_pipeline():\n",
        "    \"\"\"Complete MLOps pipeline with Databricks Unity Catalog\"\"\"\n",
        "\n",
        "    # Initialize Databricks Vector Search\n",
        "    vsc = VectorSearchClient()\n",
        "\n",
        "    # Create vector search endpoint\n",
        "    vsc.create_endpoint(\n",
        "        name=\"document-processing-endpoint\",\n",
        "        endpoint_type=\"STANDARD\"\n",
        "    )\n",
        "\n",
        "    # Create Unity Catalog managed vector index\n",
        "    index = vsc.create_delta_sync_index(\n",
        "        endpoint_name=\"document-processing-endpoint\",\n",
        "        source_table_name=\"main.documents.processed_chunks\",\n",
        "        index_name=\"main.documents.vector_index\",\n",
        "        pipeline_type=\"CONTINUOUS\",  # Auto-sync with source table\n",
        "        primary_key=\"chunk_id\",\n",
        "        embedding_source_column=\"chunk_text\",\n",
        "        embedding_model_endpoint_name=\"databricks-bge-large-en\"  # Managed embedding model\n",
        "    )\n",
        "\n",
        "    # MLflow integration for model tracking\n",
        "    import mlflow\n",
        "    from mlflow.deployments import get_deploy_client\n",
        "\n",
        "    def register_rag_model():\n",
        "        \"\"\"Register complete RAG model in MLflow\"\"\"\n",
        "\n",
        "        class RAGModel(mlflow.pyfunc.PythonModel):\n",
        "            def __init__(self, vector_index_name):\n",
        "                self.vector_index_name = vector_index_name\n",
        "                self.vsc = VectorSearchClient()\n",
        "\n",
        "            def predict(self, context, model_input):\n",
        "                \"\"\"RAG inference with vector search + LLM\"\"\"\n",
        "\n",
        "                query = model_input[\"query\"]\n",
        "\n",
        "                # Vector search\n",
        "                search_results = self.vsc.similarity_search(\n",
        "                    index_name=self.vector_index_name,\n",
        "                    query_text=query,\n",
        "                    columns=[\"chunk_text\", \"metadata\"],\n",
        "                    num_results=5\n",
        "                )\n",
        "\n",
        "                # Get context from search results\n",
        "                context_text = \"\\\\n\".join([\n",
        "                    result[\"chunk_text\"] for result in search_results[\"result\"][\"data_array\"]\n",
        "                ])\n",
        "\n",
        "                # LLM completion using Databricks Foundation Models\n",
        "                deploy_client = get_deploy_client(\"databricks\")\n",
        "\n",
        "                prompt = f\"\"\"\n",
        "                Context: {context_text}\n",
        "\n",
        "                Question: {query}\n",
        "\n",
        "                Answer based on the context provided:\n",
        "                \"\"\"\n",
        "\n",
        "                response = deploy_client.predict(\n",
        "                    endpoint=\"databricks-llama-2-70b-chat\",\n",
        "                    inputs={\"prompt\": prompt}\n",
        "                )\n",
        "\n",
        "                return response[\"choices\"][\"text\"]\n",
        "\n",
        "        # Log model with MLflow\n",
        "        with mlflow.start_run() as run:\n",
        "            rag_model = RAGModel(\"main.documents.vector_index\")\n",
        "\n",
        "            mlflow.pyfunc.log_model(\n",
        "                \"rag_model\",\n",
        "                python_model=rag_model,\n",
        "                registered_model_name=\"document_rag_model\"\n",
        "            )\n",
        "\n",
        "        return run.info.run_id\n",
        "\n",
        "    # Databricks Workflows for automation\n",
        "    def create_document_processing_workflow():\n",
        "        \"\"\"Databricks Workflow for end-to-end processing\"\"\"\n",
        "\n",
        "        workflow_spec = {\n",
        "            \"name\": \"Document Processing Pipeline\",\n",
        "            \"job_clusters\": [{\n",
        "                \"job_cluster_key\": \"processing_cluster\",\n",
        "                \"new_cluster\": {\n",
        "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
        "                    \"node_type_id\": \"i3.xlarge\",\n",
        "                    \"num_workers\": 4,\n",
        "                    \"spark_conf\": {\n",
        "                        \"spark.databricks.delta.preview.enabled\": \"true\"\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"tasks\": [\n",
        "                {\n",
        "                    \"task_key\": \"extract_documents\",\n",
        "                    \"job_cluster_key\": \"processing_cluster\",\n",
        "                    \"notebook_task\": {\n",
        "                        \"notebook_path\": \"/Repos/main/document-pipeline/extract_documents\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"task_key\": \"process_with_llamaindex\",\n",
        "                    \"depends_on\": [{\"task_key\": \"extract_documents\"}],\n",
        "                    \"job_cluster_key\": \"processing_cluster\",\n",
        "                    \"notebook_task\": {\n",
        "                        \"notebook_path\": \"/Repos/main/document-pipeline/process_llamaindex\"\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"task_key\": \"update_vector_index\",\n",
        "                    \"depends_on\": [{\"task_key\": \"process_with_llamaindex\"}],\n",
        "                    \"job_cluster_key\": \"processing_cluster\",\n",
        "                    \"notebook_task\": {\n",
        "                        \"notebook_path\": \"/Repos/main/document-pipeline/update_vectors\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Create workflow using Databricks SDK\n",
        "        from databricks.sdk import WorkspaceClient\n",
        "\n",
        "        w = WorkspaceClient()\n",
        "        job = w.jobs.create(**workflow_spec)\n",
        "\n",
        "        return job.job_id\n",
        "\n",
        "    return {\n",
        "        'vector_index': index,\n",
        "        'model_registration': register_rag_model,\n",
        "        'workflow_creation': create_document_processing_workflow\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "BSg8nxViSaHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ—ï¸ **Enterprise Architecture Integration**\n",
        "\n",
        "Here's how they work together in a production document processing pipeline:\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Document Sources] --> B[Airflow Orchestrator]\n",
        "    B --> C[Spark Cluster Processing]\n",
        "    C --> D[LlamaIndex Integration]\n",
        "    D --> E[Snowflake Data Warehouse]\n",
        "    D --> F[Databricks Vector Search]\n",
        "    E --> G[Snowflake Cortex RAG]\n",
        "    F --> H[Databricks MLOps]\n",
        "    G --> I[Production Applications]\n",
        "    H --> I\n",
        "```\n",
        "\n",
        "**Recommended Architecture**:\n",
        "\n",
        "1. **Airflow** orchestrates the entire pipeline with scheduling and monitoring[1][2]\n",
        "2. **Spark** handles large-scale document processing and preprocessing[9][7]\n",
        "3. **LlamaIndex** provides sophisticated chunking, metadata extraction, and embedding generation\n",
        "4. **Snowflake** stores structured metadata and enables native RAG with Cortex[12][14]\n",
        "5. **Databricks** manages vector indexes, model deployment, and MLOps workflows[18][20]\n",
        "\n",
        "This combination gives you:\n",
        "- **Enterprise scale** (millions of documents)\n",
        "- **Production reliability** (monitoring, retries, governance)\n",
        "- **Cost optimization** (auto-scaling, serverless options)\n",
        "- **Security compliance** (unified access control, audit trails)\n",
        "- **Operational simplicity** (managed services, unified platforms)\n",
        "\n",
        "Each tool handles what it does best while LlamaIndex remains your core document intelligence engine, creating a robust production-ready document processing ecosystem.\n"
      ],
      "metadata": {
        "id": "uoijYxZHSNPk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6C1KoapJSEOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}