{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIPmHwOin8_Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Built-in Document Loaders\n",
        "SimpleDirectoryReader**\n",
        "\n",
        "\n",
        "Supports: PDFs, Word docs, PowerPoint, images, audio, video, markdown, HTML, JSON, CSV"
      ],
      "metadata": {
        "id": "6vTnlf4in-cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# Load everything from a directory\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "# Filter by file types\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\",\n",
        "        required_exts=[\".pdf\", \".docx\", \".txt\"]\n",
        "        ).load_data()\n",
        "\n",
        "# Recursive directory scanning\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\",\n",
        "        recursive=True\n",
        "        ).load_data()\n"
      ],
      "metadata": {
        "id": "n_ba55zyn-5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Specialized Data Connectors from LlamaHub**\n",
        "\n"
      ],
      "metadata": {
        "id": "aUK4r6qOolWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Integration"
      ],
      "metadata": {
        "id": "BdfyMMSmhUWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.database import DatabaseReader\n",
        "\n",
        "# Connect to any SQL database\n",
        "reader = DatabaseReader(scheme=\"postgresql\",\n",
        "                        host=\"localhost\",port=5432,\n",
        "                        user=\"admin\",password=\"password\",\n",
        "                        dbname=\"company_db\"\n",
        "                        )\n",
        "\n",
        "# Natural language queries to SQL\n",
        "query = \"SELECT * FROM customers WHERE signup_date > '2024-01-01'\"\n",
        "documents = reader.load_data(query=query)\n"
      ],
      "metadata": {
        "id": "pkdHAq3-ox4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Workspace Integration"
      ],
      "metadata": {
        "id": "xJcGVfMEhXNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.google import GoogleDocsReader\n",
        "\n",
        "# Load Google Docs directly\n",
        "loader = GoogleDocsReader()\n",
        "gdoc_ids = ['1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec']\n",
        "documents = loader.load_data(document_ids=gdoc_ids)\n"
      ],
      "metadata": {
        "id": "R3UJLy9bo119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraping & APIs"
      ],
      "metadata": {
        "id": "7bYUaz6GhcF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "# Scrape web pages\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data([\n",
        "    \"https://example.com/page1\",\n",
        "        \"https://example.com/page2\"\n",
        "\n",
        "\"\n",
        "        ])"
      ],
      "metadata": {
        "id": "4922MfbtpB4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Advanced Preprocessing with Custom Transformations**"
      ],
      "metadata": {
        "id": "5uqI6IHUgdqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Node Parser"
      ],
      "metadata": {
        "id": "9cfE6GPIf_Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import NodeParser\n",
        "from llama_index.core.schema import Document, TextNode\n",
        "\n",
        "class CustomCodeParser(NodeParser):\n",
        "    def get_nodes_from_documents(self, documents):\n",
        "        nodes = []\n",
        "        for doc in documents:\n",
        "            # Split code by functions\n",
        "            functions = self.extract_functions(doc.text)\n",
        "            for func_name, func_code in functions:\n",
        "                node = TextNode(\n",
        "                    text=func_code,\n",
        "                    metadata={\n",
        "                        \"function_name\": func_name,\n",
        "                        \"file_path\": doc.metadata.get(\"file_path\"),\n",
        "                        \"language\": \"python\"\n",
        "                    }\n",
        "                )\n",
        "                nodes.append(node)\n",
        "        return nodes\n"
      ],
      "metadata": {
        "id": "t04ovfcjgVcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline with Custom Transformations"
      ],
      "metadata": {
        "id": "X1S_A1omganC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.extractors import TitleExtractor, QuestionsAnsweredExtractor\n",
        "\n",
        "# Custom preprocessing pipeline\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        # Custom text cleaning\n",
        "        lambda docs: [self.clean_text(doc) for doc in docs],\n",
        "\n",
        "        # Extract titles and questions automatically\n",
        "        TitleExtractor(nodes=5),\n",
        "        QuestionsAnsweredExtractor(questions=3),\n",
        "\n",
        "        # Custom metadata enrichment\n",
        "        lambda docs: self.add_custom_metadata(docs),\n",
        "\n",
        "        # Smart chunking\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "    ]\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=documents)\n"
      ],
      "metadata": {
        "id": "4MLw6Jt-ghFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Smart Chunking Strategies**"
      ],
      "metadata": {
        "id": "LTKbs7JBgjyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Document Parsing"
      ],
      "metadata": {
        "id": "mA4gx9FZgmHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import HierarchicalNodeParser\n",
        "\n",
        "# Preserve document structure\n",
        "parser = HierarchicalNodeParser.from_defaults(\n",
        "    chunk_sizes=[2048, 512, 128]  # Multi-level chunks\n",
        ")\n",
        "nodes = parser.get_nodes_from_documents(documents)\n"
      ],
      "metadata": {
        "id": "6qMlY90ggoJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code-Aware Splitting"
      ],
      "metadata": {
        "id": "M_Gb4V0Jgq5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import CodeSplitter\n",
        "\n",
        "# Split code intelligently\n",
        "splitter = CodeSplitter(\n",
        "    language=\"python\",\n",
        "    chunk_lines=40,  # Max lines per chunk\n",
        "    chunk_lines_overlap=15,\n",
        "    max_chars=1500\n",
        ")\n",
        "nodes = splitter.get_nodes_from_documents(code_documents)\n"
      ],
      "metadata": {
        "id": "MZE1F_-ngscF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Metadata Extraction & Enrichment**"
      ],
      "metadata": {
        "id": "36dfUlpNgvMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Metadata Extractors"
      ],
      "metadata": {
        "id": "hxrXmTTAgyAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors import (\n",
        "    TitleExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    SummaryExtractor,\n",
        "    KeywordExtractor\n",
        ")\n",
        "\n",
        "# Chain multiple extractors\n",
        "extractors = [\n",
        "    TitleExtractor(nodes=5),\n",
        "    QuestionsAnsweredExtractor(questions=3),\n",
        "    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
        "    KeywordExtractor(keywords=10)\n",
        "]\n",
        "\n",
        "# Apply to pipeline\n",
        "pipeline = IngestionPipeline(transformations=extractors)\n",
        "enriched_nodes = pipeline.run(documents=documents)\n"
      ],
      "metadata": {
        "id": "0B6ylK41gzle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Metadata Enrichment"
      ],
      "metadata": {
        "id": "Ky83Rs5Yg1nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_business_metadata(documents):\n",
        "    for doc in documents:\n",
        "        # Extract business-specific info\n",
        "        doc.metadata.update({\n",
        "            \"department\": extract_department(doc.text),\n",
        "            \"document_type\": classify_document(doc.text),\n",
        "            \"urgency_level\": assess_urgency(doc.text),\n",
        "            \"stakeholders\": extract_people(doc.text)\n",
        "        })\n",
        "    return documents\n"
      ],
      "metadata": {
        "id": "za50YYcIg3bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structured Data Extraction"
      ],
      "metadata": {
        "id": "8GxV3h2vwUd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "class CompanyInfo(BaseModel):\n",
        "    name: str\n",
        "    industry: str\n",
        "    key_products: List[str]\n",
        "    revenue: str\n",
        "\n",
        "# Custom prompt for extraction\n",
        "extraction_prompt = PromptTemplate(\n",
        "    \"Extract company information from the following text:\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Return the information in the specified JSON format.\"\n",
        ")\n",
        "\n",
        "output_parser = PydanticOutputParser(CompanyInfo)\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    output_parser=output_parser,\n",
        "    text_qa_template=extraction_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "8NMVua9RwZF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code analysis & QnA"
      ],
      "metadata": {
        "id": "x2NwgZcJwewg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# Use CodeLlama for code analysis\n",
        "Settings.llm = Ollama(model=\"codellama:7b\")\n",
        "\n",
        "# Load code files\n",
        "code_documents = SimpleDirectoryReader(\n",
        "    \"./src\",\n",
        "    file_extractor={\n",
        "        \".py\": \"python\",\n",
        "        \".js\": \"javascript\",\n",
        "        \".java\": \"java\"\n",
        "    }\n",
        ").load_data()\n",
        "\n",
        "# Create specialized index for code\n",
        "code_index = VectorStoreIndex.from_documents(\n",
        "    code_documents,\n",
        "    storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "oecrKG9fwiIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Real-World Integration Examples**"
      ],
      "metadata": {
        "id": "l1IA9f6gg72w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enterprise Document Processing"
      ],
      "metadata": {
        "id": "WFYCjIJXg-Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import pinecone\n",
        "\n",
        "\n",
        "# Complete enterprise pipeline\n",
        "def build_enterprise_rag():\n",
        "    # 1. Load from multiple sources\n",
        "    pdf_docs = SimpleDirectoryReader(\"./pdfs\").load_data()\n",
        "    db_docs = DatabaseReader(**db_config).load_data(query=\"SELECT * FROM documents\")\n",
        "    web_docs = SimpleWebPageReader().load_data(urls)\n",
        "\n",
        "    # 2. Custom preprocessing\n",
        "    all_docs = pdf_docs + db_docs + web_docs\n",
        "\n",
        "    # 3. Advanced pipeline\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            # OCR for scanned PDFs\n",
        "            OCRProcessor(),\n",
        "            # Clean and normalize\n",
        "            TextCleaner(),\n",
        "            # Extract structured data\n",
        "            TitleExtractor(),\n",
        "            KeywordExtractor(keywords=15),\n",
        "            # Intelligent chunking\n",
        "            SentenceSplitter(chunk_size=1024),\n",
        "            # Company-specific metadata\n",
        "            CompanyMetadataExtractor()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 4. Process and index\n",
        "    nodes = pipeline.run(documents=all_docs)\n",
        "\n",
        "    # 1. Initialize Pinecone\n",
        "    pinecone.init(\n",
        "        api_key=\"your-pinecone-api-key\",\n",
        "        environment=\"us-east-1-aws\"  # your region\n",
        "    )\n",
        "\n",
        "    # 2. Create/connect to Pinecone index\n",
        "    index_name = \"document-index\"\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "        pinecone.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1024,  # MistralAI embedding dimension\n",
        "            metric=\"cosine\"\n",
        "        )\n",
        "\n",
        "    pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "    # 3. Create PineconeVectorStore\n",
        "    vector_store = PineconeVectorStore(\n",
        "        pinecone_index=pinecone_index,\n",
        "        namespace=\"production\",  # optional\n",
        "        metadata_filters={\"category\": \"technical\"}   # # Advanced vector store setup with metadata filtering\n",
        "    )\n",
        "\n",
        "    # 4. Create StorageContext with Pinecone\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    # 5. Set embedding model (IMPORTANT: must match Pinecone dimension)\n",
        "    Settings.embed_model = MistralAIEmbeddings(\n",
        "                                model=\"mistral-embed\",\n",
        "                                api_key=mistral_key,\n",
        "                                max_retries=5,\n",
        "                                # request_timeout=60,\n",
        "                            )\n",
        "\n",
        "    # 6. Create VectorStoreIndex from your processed nodes\n",
        "    index = VectorStoreIndex(\n",
        "        nodes=nodes,  # Your processed nodes from pipeline\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Index created with {len(nodes)} nodes in Pinecone!\")\n",
        "\n",
        "    # 7. Create query engine and test\n",
        "    query_engine = index.as_query_engine(similarity_top_k=5,response_mode=\"tree_summarize\")\n",
        "    # response = query_engine.query(\"Your question here\")\n",
        "    # print(response)\n",
        "\n",
        "    return query_engine\n"
      ],
      "metadata": {
        "id": "AKjwZWe2g_ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-modal Data Extraction"
      ],
      "metadata": {
        "id": "Czo8jnBHwvpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.multi_modal_llms.ollama import OllamaMultiModal\n",
        "\n",
        "# Use multi-modal model\n",
        "mm_llm = OllamaMultiModal(model=\"llava:7b\")\n",
        "\n",
        "# Load documents with images\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./mixed_content\",\n",
        "    file_extractor={\n",
        "        \".jpg\": \"image\",\n",
        "        \".png\": \"image\",\n",
        "        \".pdf\": \"pdf\"\n",
        "    }\n",
        ").load_data()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=mm_llm)"
      ],
      "metadata": {
        "id": "_eyWSX6EtNnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Graph Construction"
      ],
      "metadata": {
        "id": "YOct4MdJw4oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.graph_stores.simple import SimpleGraphStore\n",
        "\n",
        "# Create knowledge graph\n",
        "graph_store = SimpleGraphStore()\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store,\n",
        "    graph_store=graph_store\n",
        ")\n",
        "\n",
        "kg_index = KnowledgeGraphIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    max_triplets_per_chunk=10\n",
        ")\n",
        "\n",
        "# Query the knowledge graph\n",
        "kg_query_engine = kg_index.as_query_engine(\n",
        "    include_text=False,\n",
        "    response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "response = kg_query_engine.query(\"What are the relationships between different entities?\")"
      ],
      "metadata": {
        "id": "MFr39aPXw3fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Database + Natural Language Interface"
      ],
      "metadata": {
        "id": "GWWWXVJohBm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "from llama_index.core import SQLDatabase\n",
        "\n",
        "# Connect to production database\n",
        "engine = create_engine(\"postgresql://user:pass@host/db\")\n",
        "sql_database = SQLDatabase(engine, include_tables=[\"customers\", \"orders\", \"products\"])\n",
        "\n",
        "# Natural language to SQL\n",
        "query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=[\"customers\", \"orders\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Ask business questions\n",
        "response = query_engine.query(\"Who are our top 5 customers by revenue this quarter?\")\n"
      ],
      "metadata": {
        "id": "pUcJBhGFhDE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document Comparison & Summarization"
      ],
      "metadata": {
        "id": "rXQlPKPmxL-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# Create separate indices for different document sets\n",
        "index1 = VectorStoreIndex.from_documents(doc_set_1, storage_context=storage_context)\n",
        "index2 = VectorStoreIndex.from_documents(doc_set_2, storage_context=storage_context)\n",
        "\n",
        "# Create query engine tools\n",
        "tool1 = QueryEngineTool.from_defaults(\n",
        "    query_engine=index1.as_query_engine(),\n",
        "    description=\"Contains information about Product A\"\n",
        ")\n",
        "\n",
        "tool2 = QueryEngineTool.from_defaults(\n",
        "    query_engine=index2.as_query_engine(),\n",
        "    description=\"Contains information about Product B\"\n",
        ")\n",
        "\n",
        "# Sub-question query engine for comparison\n",
        "comparison_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=[tool1, tool2]\n",
        ")\n",
        "\n",
        "response = comparison_engine.query(\"Compare Product A and Product B features\")"
      ],
      "metadata": {
        "id": "9850OOkExQlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Production-Ready Features**"
      ],
      "metadata": {
        "id": "LSQ5JxZzhFeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching & Incremental Updates"
      ],
      "metadata": {
        "id": "rOY9gFKehHka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.ingestion import IngestionCache\n",
        "\n",
        "# Persistent caching for large datasets\n",
        "cache = IngestionCache(\n",
        "    cache_dir=\"./ingestion_cache\",\n",
        "    collection=\"company_docs\"\n",
        ")\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[...],\n",
        "    cache=cache\n",
        ")\n",
        "\n",
        "# Only processes new/changed documents\n",
        "nodes = pipeline.run(documents=new_documents)\n"
      ],
      "metadata": {
        "id": "1S22V6YRhI8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Async Processing for Scale"
      ],
      "metadata": {
        "id": "IWqydoVkhLHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from llama_index.core.ingestion import arun_transformations\n",
        "\n",
        "async def process_large_dataset():\n",
        "    # Process 10,000+ documents efficiently\n",
        "    batches = chunk_documents(all_documents, batch_size=100)\n",
        "\n",
        "    tasks = []\n",
        "    for batch in batches:\n",
        "        task = arun_transformations(\n",
        "            documents=batch,\n",
        "            transformations=pipeline.transformations\n",
        "        )\n",
        "        tasks.append(task)\n",
        "\n",
        "    # Process all batches concurrently\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return flatten(results)\n"
      ],
      "metadata": {
        "id": "Imq_j00GhMci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monitoring & Debugging**"
      ],
      "metadata": {
        "id": "BGxHpkdHyC9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Enable debug logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Add callbacks for monitoring\n",
        "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
        "\n",
        "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
        "callback_manager = CallbackManager([llama_debug])\n",
        "\n",
        "Settings.callback_manager = callback_manager\n",
        "\n",
        "# Query with tracing\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Your question\")\n",
        "\n",
        "# Print event traces\n",
        "llama_debug.print_trace()"
      ],
      "metadata": {
        "id": "vboROOaLyGIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Full-fledged Enterprise Document Processing**"
      ],
      "metadata": {
        "id": "2FibUbHh0O5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pinecone\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.extractors import TitleExtractor, KeywordExtractor\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.readers.database import DatabaseReader\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "def create_pinecone_index_with_llamaindex():\n",
        "    \"\"\"Complete example: Process documents and store in Pinecone using LlamaIndex\"\"\"\n",
        "\n",
        "    # 1. Initialize Pinecone - ServerSpec or Serverless\n",
        "    pinecone.init(\n",
        "        api_key=\"your-pinecone-api-key\",\n",
        "        environment=\"us-east-1-aws\"  # or your preferred region\n",
        "    )\n",
        "\n",
        "    # 2. Create or connect to Pinecone index\n",
        "    index_name = \"document-index\"\n",
        "\n",
        "    # Check if index exists, create if not\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "        pinecone.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1024,  # MistralAI embedding dimension\n",
        "            metric=\"cosine\"\n",
        "        )\n",
        "\n",
        "    # Get the Pinecone index\n",
        "    pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "    # 3. Create PineconeVectorStore\n",
        "    vector_store = PineconeVectorStore(\n",
        "        pinecone_index=pinecone_index,\n",
        "        namespace=\"production\",  # optional namespace for organization\n",
        "        metadata_filters={\"category\": \"technical\"}   # # Advanced vector store setup with metadata filtering\n",
        "    )\n",
        "\n",
        "    # 4. Create StorageContext with Pinecone\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    # 5. Set up embedding model (important: must match Pinecone dimension)\n",
        "    embed_model = MistralAIEmbeddings(\n",
        "                      model=\"mistral-embed\",\n",
        "                      api_key=mistral_key,\n",
        "                      max_retries=5,\n",
        "                      # request_timeout=60,\n",
        "                  )\n",
        "\n",
        "    # Configure LlamaIndex settings globally\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    # 6. Load from multiple sources (your existing code)\n",
        "    pdf_docs = SimpleDirectoryReader(\"./pdfs\").load_data()\n",
        "\n",
        "    # Database configuration\n",
        "    db_config = {\n",
        "        \"scheme\": \"postgresql\",\n",
        "        \"host\": \"localhost\",\n",
        "        \"port\": 5432,\n",
        "        \"user\": \"your_user\",\n",
        "        \"password\": \"your_password\",\n",
        "        \"dbname\": \"your_db\"\n",
        "    }\n",
        "    db_docs = DatabaseReader(**db_config).load_data(query=\"SELECT * FROM documents\")\n",
        "\n",
        "    urls = [\"https://example.com/page1\", \"https://example.com/page2\"]\n",
        "    web_docs = SimpleWebPageReader().load_data(urls)\n",
        "\n",
        "    # Combine all documents\n",
        "    all_docs = pdf_docs + db_docs + web_docs\n",
        "\n",
        "    # 7. Create advanced processing pipeline (your existing code)\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            # Custom text cleaning (you'd implement these)\n",
        "            # OCRProcessor(),\n",
        "            # TextCleaner(),\n",
        "\n",
        "            # Built-in LlamaIndex extractors\n",
        "            TitleExtractor(nodes=5),\n",
        "            KeywordExtractor(keywords=15),\n",
        "\n",
        "            # Intelligent chunking\n",
        "            SentenceSplitter(\n",
        "                chunk_size=1024,\n",
        "                chunk_overlap=200\n",
        "            ),\n",
        "\n",
        "            # Custom metadata extractor (you'd implement this)\n",
        "            # CompanyMetadataExtractor()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 8. Process documents into nodes\n",
        "    nodes = pipeline.run(documents=all_docs)\n",
        "    print(f\"Processed {len(nodes)} nodes from {len(all_docs)} documents\")\n",
        "\n",
        "    # 9. Create VectorStoreIndex from processed nodes\n",
        "    index = VectorStoreIndex(\n",
        "        nodes=nodes,\n",
        "        storage_context=storage_context,\n",
        "        # embed_model is already set globally via Settings\n",
        "        show_progress=True  # Show embedding progress\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Successfully created index with {len(nodes)} nodes in Pinecone\")\n",
        "    return index\n",
        "\n",
        "def create_index_from_existing_nodes(processed_nodes):\n",
        "    \"\"\"Alternative: Create index from already processed nodes\"\"\"\n",
        "\n",
        "    # Initialize Pinecone - ServerSpec or Serverless\n",
        "    pinecone.init(\n",
        "        api_key=\"your-pinecone-api-key\",\n",
        "        environment=\"us-east-1-aws\"\n",
        "    )\n",
        "\n",
        "    # Connect to existing index\n",
        "    pinecone_index = pinecone.Index(\"document-index\")\n",
        "\n",
        "    # Create vector store and storage context\n",
        "    vector_store = PineconeVectorStore(\n",
        "        pinecone_index=pinecone_index,\n",
        "        namespace=\"production\"\n",
        "    )\n",
        "\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    # Set embedding model\n",
        "    Settings.embed_model = MistralAIEmbeddings(\n",
        "                              model=\"mistral-embed\",\n",
        "                              api_key=mistral_key,\n",
        "                              max_retries=5,\n",
        "                              # request_timeout=60,\n",
        "                          )\n",
        "\n",
        "    # Create index directly from processed nodes\n",
        "    index = VectorStoreIndex(\n",
        "        nodes=processed_nodes,  # Your processed nodes from the pipeline\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    return index\n",
        "\n",
        "def query_the_index(index):\n",
        "    \"\"\"Example of querying the created index\"\"\"\n",
        "\n",
        "    # Create query engine\n",
        "    query_engine = index.as_query_engine(\n",
        "        similarity_top_k=5,  # Return top 5 similar chunks\n",
        "        response_mode=\"tree_summarize\"  # or \"compact\", \"refine\"\n",
        "    )\n",
        "\n",
        "    # Query the index\n",
        "    response = query_engine.query(\n",
        "        \"What are the main topics discussed in the documents?\"\n",
        "    )\n",
        "\n",
        "    print(\"Query Response:\")\n",
        "    print(response)\n",
        "\n",
        "    # Access source nodes for transparency\n",
        "    print(\"\\nSource nodes:\")\n",
        "    for node in response.source_nodes:\n",
        "        print(f\"- Score: {node.score:.3f}\")\n",
        "        print(f\"  Text: {node.text[:100]}...\")\n",
        "        print(f\"  Metadata: {node.metadata}\")\n",
        "\n",
        "# Advanced configuration options\n",
        "def advanced_pinecone_configuration():\n",
        "    \"\"\"Show advanced Pinecone configuration options\"\"\"\n",
        "\n",
        "    # Initialize with custom settings - ServerSpec\n",
        "    pinecone.init(\n",
        "        api_key=\"your-pinecone-api-key\",\n",
        "        environment=\"us-east-1-aws\"\n",
        "    )\n",
        "\n",
        "    # Create index with advanced settings\n",
        "    index_name = \"advanced-document-index\"\n",
        "\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "        pinecone.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1024,\n",
        "            metric=\"cosine\",\n",
        "            pods=2,  # Scale up for better performance\n",
        "            replicas=1,  # Add replicas for high availability\n",
        "            pod_type=\"p1.x1\"  # Performance optimized pods\n",
        "        )\n",
        "\n",
        "    pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "    # Create vector store with advanced options\n",
        "    vector_store = PineconeVectorStore(\n",
        "        pinecone_index=pinecone_index,\n",
        "        namespace=\"production\",\n",
        "        text_key=\"content\",  # Custom field name for text\n",
        "        add_sparse_vector=True,  # Enable hybrid search (if supported)\n",
        "        batch_size=100  # Batch size for uploads\n",
        "    )\n",
        "\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    return storage_context\n",
        "\n",
        "def batch_processing_for_large_datasets(all_docs):\n",
        "    \"\"\"Handle large datasets efficiently\"\"\"\n",
        "\n",
        "    # Setup Pinecone as before - ServerSpec or Serverless\n",
        "    pinecone.init(api_key=\"your-key\", environment=\"us-east-1-aws\")\n",
        "    vector_store = PineconeVectorStore(\n",
        "        pinecone_index=pinecone.Index(\"document-index\"),\n",
        "        namespace=\"batch_processing\"\n",
        "    )\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    Settings.embed_model = MistralAIEmbeddings(\n",
        "                                model=\"mistral-embed\",\n",
        "                                api_key=mistral_key,\n",
        "                                max_retries=5,\n",
        "                                # request_timeout=60,\n",
        "                          )\n",
        "\n",
        "    # Process in batches to avoid memory issues\n",
        "    batch_size = 1000  # Adjust based on your memory constraints\n",
        "\n",
        "    for i in range(0, len(all_docs), batch_size):\n",
        "        batch_docs = all_docs[i:i + batch_size]\n",
        "        print(f\"Processing batch {i//batch_size + 1}: {len(batch_docs)} documents\")\n",
        "\n",
        "        # Create pipeline for this batch\n",
        "        pipeline = IngestionPipeline(\n",
        "            transformations=[\n",
        "                TitleExtractor(nodes=5),\n",
        "                KeywordExtractor(keywords=15),\n",
        "                SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Process batch\n",
        "        batch_nodes = pipeline.run(documents=batch_docs)\n",
        "\n",
        "        # Add to existing index or create new one for first batch\n",
        "        if i == 0:\n",
        "            # Create initial index\n",
        "            index = VectorStoreIndex(\n",
        "                nodes=batch_nodes,\n",
        "                storage_context=storage_context,\n",
        "                show_progress=True\n",
        "            )\n",
        "        else:\n",
        "            # Add to existing index\n",
        "            for node in batch_nodes:\n",
        "                index.insert(node)\n",
        "\n",
        "        print(f\"âœ… Batch {i//batch_size + 1} processed and added to Pinecone\")\n",
        "\n",
        "    return index\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Complete workflow example\n",
        "    index = create_pinecone_index_with_llamaindex()\n",
        "\n",
        "    # Query the created index\n",
        "    query_the_index(index)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Pinecone + LlamaIndex integration complete! ðŸš€\")\n"
      ],
      "metadata": {
        "id": "uRCQFLqLtT8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Performance Tips**\n",
        "\n",
        "    Chunking Strategy: Use semantic splitters for better context preservation\n",
        "    Embedding Models: BGE models often provide best quality/performance ratio\n",
        "    Quantization: Use 4-bit quantization for local models to reduce memory\n",
        "    Batch Processing: Process documents in batches for large datasets\n",
        "    Caching: Enable caching for frequently accessed embeddings"
      ],
      "metadata": {
        "id": "exS1bFVjy9S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open Source Models**"
      ],
      "metadata": {
        "id": "pbZriyrozWmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# BGE models (recommended)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "# or BAAI/bge-base-en-v1.5, BAAI/bge-large-en-v1.5\n",
        "\n",
        "# E5 models\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/e5-small-v2\")\n",
        "\n",
        "# UAE models\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"WhereIsAI/UAE-Large-V1\")\n",
        "\n",
        "# Nomic Embed (fully open source)\n",
        "from llama_index.embeddings.nomic import NomicEmbedding\n",
        "embed_model = NomicEmbedding(\n",
        "    api_key=\"your_nomic_api_key\",\n",
        "    model_name=\"nomic-embed-text-v1\",\n",
        "    task_type=\"search_document\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ydulba2ZzZNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 4-bit quantization for memory efficiency\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    context_window=3900,\n",
        "    max_new_tokens=256,\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "7hcgP0wFzdto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ollama Models**"
      ],
      "metadata": {
        "id": "H3Ji_xV4zi8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# First run: ollama pull mistral (or llama3.1, mixtral)\n",
        "llm = Ollama(\n",
        "    model=\"mistral\",  # or \"llama3.1\", \"mixtral\"\n",
        "    request_timeout=30.0\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "ajvXWx26zlE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistral AI**"
      ],
      "metadata": {
        "id": "VYfkPNItz7wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Set up Mistral LLM\n",
        "llm = MistralAI(\n",
        "    api_key=\"your_mistral_api_key\",\n",
        "    model=\"open-mixtral-8x22b\",  # or \"mistral-7b-instruct\"\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Set up Mistral embeddings\n",
        "embed_model = MistralAIEmbedding(\n",
        "    api_key=\"your_mistral_api_key\",\n",
        "    model_name=\"mistral-embed\"\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "FC3bEifez-Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Search**"
      ],
      "metadata": {
        "id": "kZn6D7BzyMHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=20\n",
        ")\n",
        "\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[reranker]\n",
        "    )"
      ],
      "metadata": {
        "id": "F3kfpADiyOKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced retrival**"
      ],
      "metadata": {
        "id": "ZNuS3pg7yx4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "# Custom retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=10,\n",
        ")\n",
        "\n",
        "# Post-processor to filter results\n",
        "processor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "\n",
        "# Query engine with custom retriever\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[processor],\n",
        ")"
      ],
      "metadata": {
        "id": "utKQqMVXy0WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Response Synthesis**"
      ],
      "metadata": {
        "id": "FlwFja8NyS7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "\n",
        "response_synthesizer = TreeSummarize(\n",
        "    summary_template=\"\"\"\n",
        "    Based on the context information:\n",
        "    {context_str}\n",
        "\n",
        "    Please provide a comprehensive answer to: {query_str}\n",
        "\n",
        "    Format your response with clear sections and bullet points where appropriate.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    response_synthesizer=response_synthesizer\n",
        ")"
      ],
      "metadata": {
        "id": "YRxJim8oyVzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "tDuKPGtd0Iij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core LlamaIndex\n",
        "pip install llama-index-core\n",
        "\n",
        "# For Pinecone vector store\n",
        "pip install llama-index-vector-stores-pinecone\n",
        "pip install pinecone-client\n",
        "\n",
        "# For open source models\n",
        "pip install llama-index-llms-huggingface\n",
        "pip install llama-index-llms-ollama\n",
        "pip install llama-index-llms-mistralai\n",
        "pip install llama-index-embeddings-huggingface\n",
        "\n",
        "# Optional: For quantization\n",
        "pip install transformers torch bitsandbytes"
      ],
      "metadata": {
        "id": "f0dVlyAu0E0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}