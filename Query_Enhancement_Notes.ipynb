{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVcOi38y_Xbv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Transformations for Improved Retrieval in RAG Systems\n",
        "\n",
        "### 1. Query Rewriting\n",
        "\n",
        "- **Purpose**: To make queries more specific and detailed, improving the likelihood of retrieving relevant information.\n",
        "- **Implementation**:\n",
        "  - Takes the original query and reformulates it to be more specific and detailed.\n",
        "\n",
        "### 2. Step-back Prompting\n",
        "\n",
        "- **Purpose**: To generate broader, more general queries that can help retrieve relevant background information.\n",
        "- **Implementation**:\n",
        "  - Takes the original query and generates a more general \"step-back\" query.\n",
        "\n",
        "### 3. Sub-query Decomposition\n",
        "\n",
        "- **Purpose**: To break down complex queries into simpler sub-queries for more comprehensive information retrieval.\n",
        "- **Implementation**:\n",
        "  - Decomposes the original query into 2-4 simpler sub-queries.\n",
        "\n",
        "## Benefits of these Approaches\n",
        "\n",
        "1. **Improved Relevance**: Query rewriting helps in retrieving more specific and relevant information.\n",
        "2. **Better Context**: Step-back prompting allows for retrieval of broader context and background information.\n",
        "3. **Comprehensive Results**: Sub-query decomposition enables retrieval of information that covers different aspects of a complex query.\n",
        "4. **Flexibility**: Each technique can be used independently or in combination, depending on the specific use case.\n"
      ],
      "metadata": {
        "id": "HuYrnZPwAT0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/query_transformations.ipynb"
      ],
      "metadata": {
        "id": "AYwKhuvmD7Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Query Rewriting: Reformulating queries to improve retrieval."
      ],
      "metadata": {
        "id": "uFnbdJ_mBaVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "\n",
        " 1. define llm\n",
        " 2. prompt_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
        "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
        "\n",
        "            Original query: {original_query}\n",
        " Rewritten query:\"\"\"\n",
        "\n",
        " 3. prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=prompt_template\n",
        ")\n",
        " 4. add to chain + invoke\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "47SJoN_2AaQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Step-back Prompting: Generating broader queries for better context retrieval."
      ],
      "metadata": {
        "id": "SyNJWrSxBV9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "prompt_template =\"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
        "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "Step-back query:\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "mND5fb1tBcBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Sub-query Decomposition: Breaking complex queries into simpler sub-queries.\n"
      ],
      "metadata": {
        "id": "fKlBU_okCKXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "prompt_template =\"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
        "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "example: What are the impacts of climate change on the environment?\n",
        "\n",
        "Sub-queries:\n",
        "1. What are the impacts of climate change on biodiversity?\n",
        "2. How does climate change affect the oceans?\n",
        "3. What are the effects of climate change on agriculture?\n",
        "4. What are the impacts of climate change on human health?\"\"\"\n",
        "\n",
        "after getting response back from chain.invoke(query)\n",
        "\n",
        "sub_queries = [q.strip() for q in response.content.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "FWPHm82GCSOE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6V2hmd_C9zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothetical Document Embedding (HyDE) in Document Retrieval"
      ],
      "metadata": {
        "id": "NeOxn-gFDdk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb"
      ],
      "metadata": {
        "id": "zo2d9tBRDxOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing and Vector Store Creation\n",
        "\n",
        "1. The PDF is processed and split into chunks.\n",
        "2. A Vector store is created for efficient similarity search.\n",
        "\n",
        "### Hypothetical Document Generation\n",
        "\n",
        "1. A language model is used to generate a hypothetical document that answers the given query.\n",
        "2. The generation is guided by a prompt template that ensures the hypothetical document is detailed and matches the chunk size used in the vector store.\n",
        "\n",
        "### Retrieval Process\n",
        "\n",
        "The `HyDERetriever` class implements the following steps:\n",
        "\n",
        "1. Generate a hypothetical document from the query using the language model.\n",
        "2. Use the hypothetical document as the search query in the vector store.\n",
        "3. Retrieve the most similar documents to this hypothetical document.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. Query Expansion: Transforms short queries into detailed hypothetical documents.\n",
        "2. Flexible Configuration: Allows adjustment of chunk size, overlap, and number of retrieved documents.\n",
        "3. Integration with OpenAI Models: Uses GPT-4 for hypothetical document generation and OpenAI embeddings for vector representation.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved Relevance: By expanding queries into full documents, HyDE can potentially capture more nuanced and relevant matches.\n",
        "2. Handling Complex Queries: Particularly useful for complex or multi-faceted queries that might be difficult to match directly.\n",
        "3. Adaptability: The hypothetical document generation can adapt to different types of queries and document domains.\n",
        "4. Potential for Better Context Understanding: The expanded query might better capture the context and intent behind the original question."
      ],
      "metadata": {
        "id": "R5v4J-JODx_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Define the HyDe retriever class - creating vector store, generating hypothetical document, and retrieving"
      ],
      "metadata": {
        "id": "xc-u-QTCEjAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "\n",
        "  1. prompt_template = \"\"\"You are analyzing a corporate annual report for {document_content_description}.\n",
        "\n",
        "Based on the question: '{query}'\n",
        "\n",
        "Write a brief, factual paragraph that would typically appear in an annual report answering this question. Use corporate financial language and format. Include specific numbers, percentages, or financial terms where relevant. Do not make up specific figures - use placeholder language like \"the company reported\" or \"figures show\".\n",
        "\n",
        "Focus on the type of content and language that would actually appear in financial documents, not creative or hypothetical content.\n",
        "\n",
        "Paragraph:\"\"\"\n",
        "\n",
        "  2. hyde_prompt = PromptTemplate(\n",
        "            input_variables=[\"document_content_description\",\"query\"],\n",
        "            template=prompt_template,\n",
        "        )\n",
        "\n",
        "  3.\n",
        "    input_variables = {\"query\": query,\"document_content_description\":document_content_description}\n",
        "    response = hyde_chain.invoke(input_variables)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "AtC51VjrFjcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create a HyDe retriever instance"
      ],
      "metadata": {
        "id": "ihz6vDAaGWG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "\n",
        "  1. hypothetical_doc = content part of response\n",
        "  2. similar_docs = vectorstore.similarity_search(...,namespace=namespace)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "jTT_ve9lEv_P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8764b3cIl_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothetical Prompt Embeddings (HyPE)"
      ],
      "metadata": {
        "id": "K_4FLC75Inuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb"
      ],
      "metadata": {
        "id": "CisN4bc6I11w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The PDF is loaded using `PyPDFLoader`.\n",
        "2. The text is split into chunks using `RecursiveCharacterTextSplitter` with specified chunk size and overlap.\n",
        "\n",
        "### Hypothetical Question Generation\n",
        "\n",
        "Instead of embedding raw text chunks, HyPE **generates multiple hypothetical prompts** for each chunk. These **precomputed questions** simulate user queries, improving alignment with real-world searches. This removes the need for runtime synthetic answer generation needed in techniques like HyDE.\n",
        "\n",
        "### Vector Store Creation\n",
        "\n",
        "1. Each hypothetical question is embedded using OpenAI embeddings.\n",
        "2. A vector store is built, associating **each question embedding with its original chunk**.\n",
        "3. This approach **stores multiple representations per chunk**, increasing retrieval flexibility.\n",
        "\n",
        "### Retriever Setup\n",
        "\n",
        "1. The retriever is optimized for **question-question matching** rather than direct document retrieval.\n",
        "2. The vector index enables **efficient nearest-neighbor** search over the hypothetical prompt embeddings.\n",
        "3. Retrieved chunks provide a **richer and more precise context** for downstream LLM generation.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Precomputed Hypothetical Prompts** – Improves query alignment without runtime overhead.\n",
        "2. **Multi-Vector Representation**– Each chunk is indexed multiple times for broader semantic coverage.\n",
        "3. **Efficient Retrieval** – FAISS ensures fast similarity search over the enhanced embeddings.\n",
        "4. **Modular Design** – The pipeline is easy to adapt for different datasets and retrieval settings. Additionally it's compatible with most optimizations like reranking etc.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "HyPE's effectiveness is evaluated across multiple datasets, showing:\n",
        "\n",
        "- Up to 42 percentage points improvement in retrieval precision\n",
        "- Up to 45 percentage points improvement in claim recall\n",
        "    (See full evaluation results in [preprint](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5139335))\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. **Eliminates Query-Time Overhead** – All hypothetical generation is done offline at indexing.\n",
        "2. **Enhanced Retrieval Precision** – Better alignment between queries and stored content.\n",
        "3. **Scalable & Efficient** – No addinal per-query computational cost; retrieval is as fast as standard RAG.\n",
        "4. **Flexible & Extensible** – Can be combined with advanced RAG techniques like reranking."
      ],
      "metadata": {
        "id": "2E0D5rhiI4aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Define generation of Hypothetical Prompt Embeddings\n",
        "\n",
        "    Uses the LLM to generate multiple hypothetical questions for a single chunk.\n",
        "    These questions will be used as 'proxies' for the chunk during retrieval.\n",
        "\n",
        "```python\n",
        "\n",
        "  template_prompt = PromptTemplate.from_template(\n",
        "        \"Analyze the input text and generate essential questions that, when answered, \\\n",
        "        capture the main points of the text. Each question should be one line, \\\n",
        "        without numbering or prefixes.\\n\\n \\\n",
        "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\n",
        "    )\n",
        "  1. create chain & invoke with chunk_text\n",
        "  2. results will be the set of qns that will be ingested into vector store along with chunk as proxy\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "2thXMTAYL2X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define creation and population of Vector Store + Retriver\n",
        "\n",
        "\n",
        "```python\n",
        " 1. Load PDF documents\n",
        " 2. Split documents into chunks\n",
        " 3. store into vector store -> chunk + hypothetical qns using the above function\n",
        " 4. create retriver\n",
        " ```"
      ],
      "metadata": {
        "id": "3-BuO1P8NPW1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cw12Ddm5I5Gx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}