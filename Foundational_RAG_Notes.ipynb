{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tkCz5MvFA-Rl",
        "kUsrtrafVrhT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoyShIF05DUg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow of a Reliable RAG\n",
        "\n",
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/reliable_rag.ipynb\n"
      ],
      "metadata": {
        "id": "tkCz5MvFA-Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Start** → query → **Vectorstore** → retrieved docs + query → **Check Document Relevancy** → relevant docs + query → **Generate Answer** → relevant docs + answer → **Check Hallucination** → query + relevant docs + answer → **Highlight Document Snippet** → **End**\n",
        "\n",
        "| Step | Input                          | Process                        | Output                         |\n",
        "| ---- | ------------------------------ | ------------------------------ | ------------------------------ |\n",
        "| 1    | –                              | **Start**                      | query                          |\n",
        "| 2    | query                          | **Vectorstore**                | retrieved docs + query         |\n",
        "| 3    | retrieved docs + query         | **Check Document Relevancy**   | relevant docs + query          |\n",
        "| 4    | relevant docs + query          | **Generate Answer**            | relevant docs + answer         |\n",
        "| 5    | relevant docs + answer         | **Check Hallucination**        | query + relevant docs + answer |\n",
        "| 6    | query + relevant docs + answer | **Highlight Document Snippet** | snippet                        |\n",
        "| 7    | snippet                        | **End**                        | –                              |\n",
        "\n",
        "\n",
        "\n",
        "### **1. Check document relevancy**\n",
        "\n",
        "```python\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "```\n",
        "\n",
        "    llm.with_structured_output(DataModel)\n",
        "\n",
        "    grader_prompt = \"system\" + (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "\n",
        "    retrieval_grader = grade_prompt | structured_llm_grader\n",
        "\n",
        "### **2. Filter out the non-relevant docs**\n",
        "\n",
        "  \n",
        "  ```\n",
        "  invoke retrieval_grader with  question + doc.page_content\n",
        "  -> res.binary_score == 'yes' put into docs_to_use list\n",
        "  ```\n",
        "\n",
        "### **3. Generate Result vs Baseline Result**\n",
        "\n",
        "  ```python\n",
        "\n",
        "  def format_docs(docs):\n",
        "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
        "\n",
        "  rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question})\n",
        "  ```\n",
        "\n",
        "### **4. Check for Hallucinations**\n",
        "\n",
        "  ```python\n",
        "  class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        ...,\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "    hallucination_prompt = \"system\" + (\"human\", \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\")\n",
        "\n",
        "    hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "\n",
        "    hallucination_grader.invoke({\"documents\": format_docs(docs_to_use), \"generation\": generation})\n",
        "  \n",
        "  ```\n",
        "\n",
        "### **5. Highlight used docs**\n",
        "\n",
        "  ```python\n",
        "  \n",
        "  class HighlightDocuments(BaseModel):\n",
        "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
        "\n",
        "    id: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of id of docs used to answers the question\"\n",
        "    )\n",
        "\n",
        "    title: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of titles used to answers the question\"\n",
        "    )\n",
        "\n",
        "    source: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of sources used to answers the question\"\n",
        "    )\n",
        "\n",
        "    segment: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of direct segements from used documents that answers the question\"\n",
        "    )\n",
        "\n",
        "\n",
        "    parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
        "\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "            template= system,\n",
        "            input_variables=[\"documents\", \"question\", \"generation\"],\n",
        "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "          )\n",
        "\n",
        "    doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})\n",
        "  \n",
        "  ```\n"
      ],
      "metadata": {
        "id": "y88Wd5dt5IRD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEQM4ChrMyNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Optimizing Chunk Sizes\n",
        "\n",
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/choose_chunk_size.ipynb"
      ],
      "metadata": {
        "id": "kUsrtrafVrhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **1. Create evaluation questions and pick k out of them**\n",
        "\n",
        "  ```python\n",
        "  num_eval_questions = 25\n",
        "\n",
        "eval_documents = documents[0:20]\n",
        "data_generator = DatasetGenerator.from_documents(eval_documents) # List[llama_index.core.schema.Document] : size = 20\n",
        "eval_questions = data_generator.generate_questions_from_nodes()  # List[str] : size = 938\n",
        "k_eval_questions = random.sample(eval_questions, num_eval_questions) # List[str] : size = 25\n",
        "  \n",
        "  ```\n",
        "\n",
        "### **2. Define metrics evaluators and modify llama_index faithfullness evaluator prompt to rely on the context**\n",
        "\n",
        "```python\n",
        "\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "\n",
        "*   Set appropriate settings for the LLM\n",
        "*   Define Faithfulness Evaluators : faithfulness_eval\n",
        "*   Define Relevancy Evaluators : relevancy_eval # no prompt\n",
        "*   faithfulness_new_prompt_template :\n",
        "        ...... (few-shot prompting examples)......\n",
        "              Information: {query_str}\n",
        "              Context: {context_str}\n",
        "              Answer:\n",
        "\n",
        "\n",
        "faithfulness_eval.update_prompts({\"some_prompt_key\": faithfulness_new_prompt_template})\n",
        "\n",
        "```\n",
        "\n",
        "### **3. Function to evaluate metrics for each chunk size**\n",
        "\n",
        "```python\n",
        "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
        "  \"\"\"Evaluate the average response time, faithfulness, and relevancy of responses generated by LLM for a given chunk size.\"\"\"\n",
        "\n",
        "  1. set llm,chunk_size,chunk_overlap using Settings.\n",
        "  2. vector_index = VectorStoreIndex.from_documents(eval_documents)\n",
        "  3. build query engine with similarity_top_k=5\n",
        "  4. num_questions = len(eval_questions) # 938\n",
        "  5. for each question in eval_question:\n",
        "            query_engine.query(question)\n",
        "            faithfulness_eval.evaluate_response(res_vect).passing\n",
        "            # same for relevancy_eval\n",
        "\n",
        "    average_ = total / num_questions\n",
        "```\n",
        "\n",
        "### **4. Test different chunk sizes**\n",
        "  ```python\n",
        "  \n",
        "  chunk_sizes = [128, 256]\n",
        "\n",
        "  evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
        "\n",
        "  ```"
      ],
      "metadata": {
        "id": "7xpXw2zMpbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZy4aEy8C3id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Propositions Chunking\n",
        "\n",
        "\n",
        "https://github.com/NirDiamant/RAG_TECHNIQUES/blob/main/all_rag_techniques/proposition_chunking.ipynb"
      ],
      "metadata": {
        "id": "FE3MFM5sC2YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Document Chunking:** Splitting a document into manageable pieces for analysis.\n",
        "2. **Proposition Generation:** Using LLMs to break down document chunks into factual, self-contained propositions.\n",
        "3. **Proposition Quality Check:** Evaluating generated propositions based on accuracy, clarity, completeness, and conciseness.\n",
        "4. **Embedding and Vector Store:** Embedding both propositions and larger chunks of the document into a vector store for efficient retrieval.\n",
        "5. **Retrieval and Comparison:** Testing the retrieval system with different query sizes and comparing results from the proposition-based model with the larger chunk-based model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcmMtPFqBCRG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4j8Pk7-sC4UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Basic Chunking**\n",
        "\n",
        "  ```python\n",
        "  Build Index -> Set Embedding model -> create docs_list -> Split docs_list -> add 'chunk_id' to metadata\n",
        "  \n",
        "  ```\n",
        "\n",
        "### **2. Generate Propositions**\n",
        "\n",
        "```python\n",
        "class GeneratePropositions(BaseModel):\n",
        "    \"\"\"List of all the propositions in a given document\"\"\"\n",
        "\n",
        "    propositions: List[str] = Field(\n",
        "        description=\"List of propositions (factual, self-contained, and concise information)\"\n",
        "    )\n",
        "\n",
        "    1. LLM with function call\n",
        "    2. Few shot prompting\n",
        "        proposition_examples = [{\"document\":  \"...\",\n",
        "     \"propositions\": \"['...', '...', '...',...]\"\n",
        "     },]\n",
        "\n",
        "    3. example_proposition_prompt = [\n",
        "        (\"human\", \"{document}\"),\n",
        "        (\"ai\", \"{propositions}\"),\n",
        "    ]\n",
        "\n",
        "    4. few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt = example_proposition_prompt,\n",
        "    examples = proposition_examples,\n",
        ")\n",
        "    5. prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{document}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "    6. proposition_generator = prompt | structured_llm\n",
        "\n",
        "    7. invoke with ({\"document\": doc_splits[i].page_content}) for each doc in docs_splits\n",
        "\n",
        "      7.1 store all response.propositions as Document(page_content=proposition, metadata={\"Title\":\"...\", \"chunk_id\": i+1}) into a list : propositions\n",
        "\n",
        "```\n",
        "\n",
        "### **3. Quality Check**\n",
        "\n",
        "```python\n",
        "class GradePropositions(BaseModel):\n",
        "    \"\"\"Grade a given proposition on accuracy, clarity, completeness, and conciseness\"\"\"\n",
        "\n",
        "    accuracy: int = Field(\n",
        "        description=\"Rate from 1-10 based on how well the proposition reflects the original text.\"\n",
        "    )\n",
        "    \n",
        "    clarity: int = Field(\n",
        "        description=\"Rate from 1-10 based on how easy it is to understand the proposition without additional context.\"\n",
        "    )\n",
        "\n",
        "    completeness: int = Field(\n",
        "        description=\"Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\"\n",
        "    )\n",
        "\n",
        "    conciseness: int = Field(\n",
        "        description=\"Rate from 1-10 based on whether the proposition is concise without losing important information.\"\n",
        "    )\n",
        "\n",
        "    1. LLM with function call\n",
        "    2. evaluation_prompt_template = \"\"\"\n",
        "        Please evaluate the following proposition based on the criteria below:\n",
        "      - **Accuracy**: Rate from 1-10 ...\n",
        "      - **Clarity**: Rate from 1-10 ...\n",
        "      - **Completeness**: ...\n",
        "      - **Conciseness**: ...\n",
        "\n",
        "      Example:\n",
        "      Docs: ...\n",
        "\n",
        "      Propositons_1: Neil Armstrong was an astronaut.\n",
        "      Evaluation_1: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
        "\n",
        "      ## similarly more examples\n",
        "\n",
        "      Format:\n",
        "      Proposition: \"{proposition}\"\n",
        "      Original Text: \"{original_text}\"\n",
        "    \n",
        "    \"\"\"\n",
        "    2. prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", evaluation_prompt_template),\n",
        "        (\"human\", \"{proposition}, {original_text}\"),\n",
        "    ]\n",
        ")\n",
        "    3. create proposition_evaluator\n",
        "    4. Define evaluation categories and thresholds\n",
        "\n",
        "    evaluation_categories = [\"accuracy\", \"clarity\", \"completeness\", \"conciseness\"]\n",
        "\n",
        "    thresholds = {\"accuracy\": 7, \"clarity\": 7, \"completeness\": 7, \"conciseness\": 7}\n",
        "\n",
        "    5. def evaluate_proposition(proposition, original_text):\n",
        "\n",
        "      5.1 invoke with {\"proposition\": proposition, \"original_text\": original_text}\n",
        "      5.2. Parse the response to extract scores : return {\"accuracy\": response.accuracy, ...}\n",
        "\n",
        "    6. def passes_quality_check(scores):\n",
        "\n",
        "      6.1 Check if the proposition passes the quality check score > thresholds[category] : return True\n",
        "\n",
        "    7.1 call 5 & 6 with evaluate_proposition(proposition.page_content, doc_splits[proposition.metadata['chunk_id'] - 1].page_content)\n",
        "    7.2 passes_quality_check(scores) -> store in evaluated_propositions=[]\n",
        "\n",
        "```\n",
        "### **4. Index into vector store & compare retrival**\n",
        "\n",
        "```python\n",
        "  1. crerate vectorstore_propositions & vectorstore_larger\n",
        "  2. create retriver for both of them\n",
        "  3. run both retriver -> get doc.page_content,doc.metadata\n",
        "  4. compare both should get following result\n",
        "\n",
        "```\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| **Aspect**                | **Proposition-Based Retrieval**                                         | **Simple Chunk Retrieval**                                              |\n",
        "|---------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|\n",
        "| **Precision in Response**  | High: Delivers focused and direct answers.                              | Medium: Provides more context but may include irrelevant information.    |\n",
        "| **Clarity and Brevity**    | High: Clear and concise, avoids unnecessary details.                    | Medium: More comprehensive but can be overwhelming.                      |\n",
        "| **Contextual Richness**    | Low: May lack context, focusing on specific propositions.               | High: Provides additional context and details.                           |\n",
        "| **Comprehensiveness**      | Low: May omit broader context or supplementary details.                 | High: Offers a more complete view with extensive information.            |\n",
        "| **Narrative Flow**         | Medium: Can be fragmented or disjointed.                                | High: Preserves the logical flow and coherence of the original document. |\n",
        "| **Information Overload**   | Low: Less likely to overwhelm with excess information.                  | High: Risk of overwhelming the user with too much information.           |\n",
        "| **Use Case Suitability**   | Best for quick, factual queries.                                        | Best for complex queries requiring in-depth understanding.               |\n",
        "| **Efficiency**             | High: Provides quick, targeted responses.                               | Medium: May require more effort to sift through additional content.      |\n",
        "| **Specificity**            | High: Precise and targeted responses.                                   | Medium: Answers may be less targeted due to inclusion of broader context.|\n"
      ],
      "metadata": {
        "id": "jLNRiP9cDLPe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3A7sXrU8J5lF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}