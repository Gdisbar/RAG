{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f371c032",
   "metadata": {},
   "source": [
    "## src/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d3770",
   "metadata": {},
   "source": [
    "```python\n",
    "from src.config import get_settings\n",
    "from src.db.factory import make_database\n",
    "\n",
    "# from src.services.arxiv.factory import make_arxiv_client\n",
    "# from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "\n",
    "# from src.routers import papers\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    app.state.settings = get_settings()\n",
    "    database = make_database()\n",
    "    app.state.database = database\n",
    "    app.state.arxiv_client = make_arxiv_client()\n",
    "    app.state.pdf_parser = make_pdf_parser_service()\n",
    "    \n",
    "    yield\n",
    "    database.teardown()\n",
    "\n",
    "# Include routers\n",
    "ping.router, prefix=\"/api/v1\"\n",
    "papers.router, prefix=\"/api/v1\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5eb85f",
   "metadata": {},
   "source": [
    "## src/routers/\n",
    "\n",
    "**- src/routers/papers.py**\n",
    "\n",
    "```routers -> schema & repositories```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675a034",
   "metadata": {},
   "source": [
    "### src/routers/papers.py\n",
    "\n",
    "```routers -> model from schemas.arxiv.paper,repositories.paper,models.paper```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a14e66",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222e4a4",
   "metadata": {},
   "source": [
    "```python\n",
    "from src.schemas.arxiv.paper import PaperResponse, PaperSearchResponse\n",
    "\n",
    "\"\"\"\n",
    "schemas/arxiv/paper.py : contains following Model(s)\n",
    "\n",
    "class ArxivPaper(BaseModel):\n",
    "    ...\n",
    "class PaperBase(BaseModel):\n",
    "    ...\n",
    "class PaperCreate(PaperBase):\n",
    "    ...\n",
    "class PaperResponse(PaperBase):\n",
    "    ...\n",
    "class PaperSearchResponse(BaseModel):\n",
    "    ...\n",
    "\"\"\"\n",
    "from src.dependencies import SessionDep\n",
    "\"\"\"\n",
    "dependencies -> call : request.app.state.database\n",
    "\n",
    "Get settings from the request state.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from src.repositories.paper import PaperRepository\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5604085",
   "metadata": {},
   "source": [
    "##### src/repositories/paper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf626a",
   "metadata": {},
   "source": [
    "```python\n",
    "from src.models.paper import Paper\n",
    "\n",
    "class Paper(Base):\n",
    "    __tablename__ = \"papers\"\n",
    "\n",
    "    # Core arXiv metadata\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    arxiv_id = Column(String, unique=True, nullable=False, index=True)\n",
    "    ...\n",
    "    # Parsed PDF content (added for comprehensive storage)\n",
    "    # PDF processing metadata\n",
    "    # Timestamps\n",
    "    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))\n",
    "    updated_at = Column(DateTime, default=lambda: datetime.now(timezone.utc), onupdate=lambda: datetime.now(timezone.utc))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7481c",
   "metadata": {},
   "source": [
    "```python\n",
    "# from src.models.paper import Paper\n",
    "# from src.schemas.arxiv.paper import PaperCreate\n",
    "\n",
    "class PaperRepository:\n",
    "    def __init__(self, session: Session):\n",
    "        self.session = session # sqlalchemy.orm\n",
    "    def create(self, paper: PaperCreate) -> Paper:\n",
    "        db_paper = Paper(**paper.model_dump())\n",
    "        # add to session -> commit -> refresh\n",
    "        return db_paper\n",
    "    def get_by_arxiv_id(self, arxiv_id: str) -> Optional[Paper]:\n",
    "        # query stmt\n",
    "        return self.session.scalar(stmt)\n",
    "    def get_by_id(self, paper_id: UUID) -> Optional[Paper]:\n",
    "        # similar to get_by_arxiv_id\n",
    "    def get_all(self, limit: int = 100, offset: int = 0) -> List[Paper]:\n",
    "        stmt = select(Paper).order_by(Paper.published_date.desc()).limit(limit).offset(offset)\n",
    "        return list(self.session.scalars(stmt))\n",
    "    def get_count(self) -> int:\n",
    "        stmt = select(func.count(Paper.id))\n",
    "        return self.session.scalar(stmt) or 0\n",
    "\n",
    "    def get_processed_papers(self, limit: int = 100, offset: int = 0) -> List[Paper]:\n",
    "        \"\"\"Get papers that have been successfully processed with PDF content.\"\"\"\n",
    "        stmt = (\n",
    "            select(Paper)\n",
    "            .where(Paper.pdf_processed == True)\n",
    "            .order_by(Paper.pdf_processing_date.desc())\n",
    "            .limit(limit)\n",
    "            .offset(offset)\n",
    "        )\n",
    "        return list(self.session.scalars(stmt))\n",
    "    \n",
    "    def get_unprocessed_papers(self, limit: int = 100, offset: int = 0) -> List[Paper]:\n",
    "        \"\"\"Get papers that haven't been processed for PDF content yet.\"\"\"\n",
    "        stmt = select(Paper).where(Paper.pdf_processed == False).order_by(Paper.published_date.desc()).limit(limit).offset(offset)\n",
    "        return list(self.session.scalars(stmt))\n",
    "    \n",
    "    def get_papers_with_raw_text(self, limit: int = 100, offset: int = 0) -> List[Paper]:\n",
    "        \"\"\"Get papers that have raw text content stored. \n",
    "        i.e Paper.raw_text != None\n",
    "        \"\"\"\n",
    "    def get_processing_stats(self) -> dict:\n",
    "        # Count processed papers\n",
    "        # Count papers with text\n",
    "        return {\n",
    "            \"total_papers\": total_papers,\n",
    "            \"processed_papers\": processed_papers,\n",
    "            \"papers_with_text\": papers_with_text,\n",
    "            \"processing_rate\": (processed_papers / total_papers * 100) if total_papers > 0 else 0,\n",
    "            \"text_extraction_rate\": (papers_with_text / processed_papers * 100) if processed_papers > 0 else 0,\n",
    "        }\n",
    "    def update(self, paper: Paper) -> Paper:\n",
    "        # add to session -> commit -> refresh\n",
    "        return paper\n",
    "    \n",
    "    def upsert(self, paper_create: PaperCreate) -> Paper:\n",
    "        # Check if paper already exists\n",
    "        existing_paper = self.get_by_arxiv_id(paper_create.arxiv_id)\n",
    "        if existing_paper:\n",
    "            # Update existing paper with new content\n",
    "            for key, value in paper_create.model_dump(exclude_unset=True).items():\n",
    "                setattr(existing_paper, key, value)\n",
    "            return self.update(existing_paper)\n",
    "        else:\n",
    "            # Create new paper\n",
    "            return self.create(paper_create)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d1699",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d1062",
   "metadata": {},
   "source": [
    "```python\n",
    "# from src.dependencies import SessionDep\n",
    "# from src.repositories.paper import PaperRepository\n",
    "# from src.schemas.arxiv.paper import PaperResponse, PaperSearchResponse\n",
    "\n",
    "router = APIRouter(prefix=\"/papers\", tags=[\"papers\"])\n",
    "\n",
    "@router.get(\"/\", response_model=PaperSearchResponse)\n",
    "def list_papers(\n",
    "    db: SessionDep,\n",
    "    limit: int = Query(default=10, ge=1, le=100, description=\"Number of papers to return (1-100)\"),\n",
    "    offset: int = Query(default=0, ge=0, description=\"Number of papers to skip\"),\n",
    ") -> PaperSearchResponse:\n",
    "    \"\"\"Get a list of papers with pagination.\"\"\"\n",
    "    paper_repo = PaperRepository(db)\n",
    "    papers = paper_repo.get_all(limit=limit, offset=offset)\n",
    "    # Get total count for pagination info\n",
    "    total = paper_repo.get_count()\n",
    "    return PaperSearchResponse(papers=[PaperResponse.model_validate(paper) for paper in papers], total=total)\n",
    "\n",
    "@router.get(\"/{arxiv_id}\", response_model=PaperResponse)\n",
    "def get_paper_details(\n",
    "    db: SessionDep,\n",
    "    arxiv_id: str = Path(\n",
    "        ..., description=\"arXiv paper ID (e.g., '2401.00001' or '2401.00001v1')\", regex=r\"^\\d{4}\\.\\d{4,5}(v\\d+)?$\"\n",
    "    ),\n",
    ") -> PaperResponse:\n",
    "    \"\"\"Get details of a specific paper by arXiv ID.\"\"\"\n",
    "    paper_repo = PaperRepository(db)\n",
    "    paper = paper_repo.get_by_arxiv_id(arxiv_id)\n",
    "    # paper not found - HTTPException(status_code=404)\n",
    "    return PaperResponse.model_validate(paper)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccdf64",
   "metadata": {},
   "source": [
    "## src/services/\n",
    "\n",
    "**- src/services/arxiv/factory.py**\n",
    "\n",
    "**- src/services/pdf_parser/factory.py**\n",
    "\n",
    "```arxiv service -> own factory methods```\n",
    "\n",
    "```pdf_parser service -> own factory methods```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177418f",
   "metadata": {},
   "source": [
    "## src/services/arxiv/\n",
    "\n",
    "**- src/services/arxiv/factory.py**\n",
    "\n",
    "**- src/services/arxiv/client.py**\n",
    "\n",
    "```arxiv service -> factory method -> client```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4412ee",
   "metadata": {},
   "source": [
    "### src/services/arxiv/factory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc428c",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# from .client import ArxivClient\n",
    "\n",
    "def make_arxiv_client() -> ArxivClient:\n",
    "    \"\"\"Factory function to create an arXiv client instance.\"\"\"\n",
    "    client = ArxivClient(settings=settings.arxiv)\n",
    "\n",
    "    return client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2829281",
   "metadata": {},
   "source": [
    "### src/services/arxiv/client.py\n",
    "\n",
    "```arxiv service -> model from schemas.arxiv.paper```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ba8cb",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843ae98",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from src.schemas.arxiv.paper import ArxivPaper\n",
    "\n",
    "\"\"\"\n",
    "schemas/arxiv/paper.py : contains following Model(s)\n",
    "\n",
    "class ArxivPaper(BaseModel):\n",
    "    ...\n",
    "class PaperBase(BaseModel):\n",
    "    ...\n",
    "class PaperCreate(PaperBase):\n",
    "    ...\n",
    "class PaperResponse(PaperBase):\n",
    "    ...\n",
    "class PaperSearchResponse(BaseModel):\n",
    "    ...\n",
    "\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46a716",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818be802",
   "metadata": {},
   "source": [
    "```python\n",
    "# from src.schemas.arxiv.paper import ArxivPaper\n",
    "\n",
    "class ArxivClient:\n",
    "    \"\"\"Client for fetching papers from arXiv API.\"\"\"\n",
    "    def __init__(self, settings: ArxivSettings):\n",
    "        self._settings = settings\n",
    "        self._last_request_time: Optional[float] = None\n",
    "\n",
    "    @cached_property\n",
    "    def pdf_cache_dir(self) -> Path:\n",
    "         \"\"\"PDF cache directory.\"\"\"\n",
    "    # getter methods as @property\n",
    "    async def fetch_papers(\n",
    "        self,\n",
    "        max_results: Optional[int] = None,\n",
    "        start: int = 0,\n",
    "        sort_by: str = \"submittedDate\",\n",
    "        sort_order: str = \"descending\",\n",
    "        from_date: Optional[str] = None,\n",
    "        to_date: Optional[str] = None,\n",
    "    ) -> List[ArxivPaper]:\n",
    "        \"\"\"\n",
    "        Fetch papers from arXiv for the configured category.\"\"\"\n",
    "        # Build basic search query\n",
    "        # Add date filtering if provided - arXiv format\n",
    "        # add additional params in basic search query\n",
    "        try:\n",
    "            # Add rate limiting delay between all requests (arXiv recommends 3 seconds)\n",
    "            if self._last_request_time is not None:\n",
    "                time_since_last = time.time() - self._last_request_time\n",
    "                if time_since_last < self.rate_limit_delay:\n",
    "                    sleep_time = self.rate_limit_delay - time_since_last\n",
    "                    await asyncio.sleep(sleep_time)\n",
    "            self._last_request_time = time.time()\n",
    "            async with httpx.AsyncClient(timeout=self.timeout_seconds) as client:\n",
    "                response = await client.get(url)\n",
    "                # get response_code,xml_data\n",
    "            papers = self._parse_response(xml_data)\n",
    "            return papers\n",
    "        except (httpx.TimeoutException,httpx.HTTPStatusError) as e:\n",
    "            # raise proper exception\n",
    "\n",
    "    def _parse_response(self, xml_data: str) -> List[ArxivPaper]:\n",
    "        \"\"\"\n",
    "        Parse arXiv API XML response into ArxivPaper objects.\"\"\"\n",
    "        try:\n",
    "            root = ET.fromstring(xml_data)\n",
    "            entries = root.findall(\"atom:entry\", self.namespaces)\n",
    "\n",
    "            papers = []\n",
    "            for entry in entries:\n",
    "                paper = self._parse_single_entry(entry)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "\n",
    "            return papers\n",
    "        except (ET.ParseError,Exception) as e:\n",
    "            # raise proper exception\n",
    "    \n",
    "     def _parse_single_entry(self, entry: ET.Element) -> Optional[ArxivPaper]:\n",
    "        \"\"\"Parse a single entry from arXiv XML response.\"\"\"\n",
    "        try:\n",
    "            # get the values\n",
    "            return ArxivPaper(\n",
    "                arxiv_id=arxiv_id,\n",
    "                title=title,\n",
    "                authors=authors,\n",
    "                abstract=abstract,\n",
    "                published_date=published,\n",
    "                categories=categories,\n",
    "                pdf_url=pdf_url,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _get_pdf_url(self, entry: ET.Element) -> str:\n",
    "        \"\"\"Extract PDF URL from entry links.\"\"\"\n",
    "        for link in entry.findall(\"atom:link\", self.namespaces):\n",
    "            if link.get(\"type\") == \"application/pdf\":\n",
    "                url = link.get(\"href\", \"\")\n",
    "                # Convert HTTP to HTTPS for arXiv URLs\n",
    "                if url.startswith(\"http://arxiv.org/\"):\n",
    "                    url = url.replace(\"http://arxiv.org/\", \"https://arxiv.org/\")\n",
    "                return url\n",
    "        return \"\"\n",
    "    \n",
    "    async def download_pdf(self, paper: ArxivPaper, force_download: bool = False) -> Optional[Path]:\n",
    "        \"\"\"Download PDF for a given paper to local cache.\"\"\"\n",
    "        # null check -> paper.pdf_url\n",
    "        pdf_path = self._get_pdf_path(paper.arxiv_id)\n",
    "        # Return cached PDF -> pdf_path.exists()\n",
    "        if await self._download_with_retry(paper.pdf_url, pdf_path):\n",
    "            return pdf_path\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    async def _download_with_retry(self, url: str, path: Path, max_retries: Optional[int] = None) -> bool:\n",
    "        \"\"\"Download a file with retry logic.\"\"\"\n",
    "        # Respect rate limits\n",
    "        await asyncio.sleep(self.rate_limit_delay)\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async with httpx.AsyncClient(timeout=float(self.timeout_seconds)) as client:\n",
    "                    async with client.stream(\"GET\", url) as response:\n",
    "                        response.raise_for_status()\n",
    "                        with open(path, \"wb\") as f:\n",
    "                            async for chunk in response.aiter_bytes():\n",
    "                                f.write(chunk)\n",
    "                return True\n",
    "            except httpx.TimeoutException as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = self._settings.download_retry_delay_base * (attempt + 1)\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    # raise Timeout\n",
    "            except httpx.HTTPError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = self._settings.download_retry_delay_base * (attempt + 1)  # Exponential backoff\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    # raise DownloadError\n",
    "            except Exception as e:\n",
    "                # raise DownloadError\n",
    "\n",
    "        # Clean up partial download\n",
    "        if path.exists():\n",
    "            path.unlink()\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    async def fetch_papers_with_query(\n",
    "        self,\n",
    "        search_query: str,\n",
    "        max_results: Optional[int] = None,\n",
    "        start: int = 0,\n",
    "        sort_by: str = \"submittedDate\",\n",
    "        sort_order: str = \"descending\",\n",
    "    ) -> List[ArxivPaper]:\n",
    "        \"\"\"same as fetch_papers() but with different query\"\"\" \n",
    "\n",
    "\n",
    "    async def fetch_paper_by_id(self, arxiv_id: str) -> Optional[ArxivPaper]:\n",
    "        \"\"\"same as fetch_papers() but less query & no ae limiting\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606a39b",
   "metadata": {},
   "source": [
    "## src/services/pdf_parser/\n",
    "\n",
    "**- src/services/pdf_parser/factory.py**\n",
    "\n",
    "**- src/services/pdf_parser/parser.py**\n",
    "\n",
    "**- src/services/pdf_parser/docling.py**\n",
    "\n",
    "```pdf_parser service -> factory method -> parser & docling```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab2e9a",
   "metadata": {},
   "source": [
    "### src/services/pdf_parser/factory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c987e",
   "metadata": {},
   "source": [
    "```python\n",
    "# from .parser import PDFParserService\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def make_pdf_parser_service() -> PDFParserService:\n",
    "    \"\"\"Create cached PDF parser service using Docling.\"\"\"\n",
    "    settings = get_settings()\n",
    "    return PDFParserService(\n",
    "        max_pages=settings.pdf_parser.max_pages,\n",
    "        max_file_size_mb=settings.pdf_parser.max_file_size_mb,\n",
    "        do_ocr=settings.pdf_parser.do_ocr,\n",
    "        do_table_structure=settings.pdf_parser.do_table_structure,\n",
    "    )\n",
    "\n",
    "def reset_pdf_parser() -> None:\n",
    "    \"\"\"\n",
    "    Reset the cached instance using lru_cache's built-in cache management.\n",
    "    Useful for testing or when configuration changes.\n",
    "    \"\"\"\n",
    "    make_pdf_parser_service.cache_clear()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61809a",
   "metadata": {},
   "source": [
    "### src/services/pdf_parser/parser.py\n",
    "\n",
    "``` pdf_parser service -> schemas.pdf_parser model & DoclingParser from docling```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344b81d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a311434",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from src.schemas.pdf_parser.models import PdfContent\n",
    "\n",
    "\"\"\"\n",
    "class ParserType(str, Enum):\n",
    "    DOCLING = \"docling\"\n",
    "\n",
    "class PaperSection(BaseModel):\n",
    "    ...\n",
    "class PaperFigure(BaseModel):\n",
    "    ...\n",
    "class PaperTable(BaseModel):\n",
    "    ...\n",
    "class PdfContent(BaseModel):\n",
    "    ...\n",
    "class ArxivMetadata(BaseModel):\n",
    "    ...\n",
    "class ParsedPaper(BaseModel):\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "from .docling import DoclingParser\n",
    "\n",
    "class DoclingParser:\n",
    "    def __init__(self, max_pages: int, max_file_size_mb: int, do_ocr: bool = False, do_table_structure: bool = True):\n",
    "        # Configure pipeline options\n",
    "        pipeline_options = PdfPipelineOptions(...)\n",
    "        self._converter = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)})\n",
    "        ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82fa73",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9f3db",
   "metadata": {},
   "source": [
    "```python\n",
    "# from src.schemas.pdf_parser.models import PdfContent\n",
    "# from .docling import DoclingParser\n",
    "\n",
    "class PDFParserService:\n",
    "    def __init__(self, max_pages: int = 20, max_file_size_mb: int = 20,do_ocr: bool = False, do_table_structure: bool = True):\n",
    "        self.docling_parser = DoclingParser(\n",
    "            max_pages=max_pages, max_file_size_mb=max_file_size_mb, do_ocr=do_ocr, do_table_structure=do_table_structure\n",
    "        )\n",
    "\n",
    "    async def parse_pdf(self, pdf_path: Path) -> Optional[PdfContent]:\n",
    "        # raise error if not pdf_path.exists()\n",
    "        try:\n",
    "            result = await self.docling_parser.parse_pdf(pdf_path)\n",
    "            if result:\n",
    "                return result\n",
    "            else:\n",
    "               # raise erroe\n",
    "        except (PDFValidationError, PDFParsingException):\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # raise error\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11631774",
   "metadata": {},
   "source": [
    "### src/services/pdf_parser/docling.py\n",
    "\n",
    "```pdf_parser service -> model from schemas.pdf_parser.models```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87064933",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b1b993",
   "metadata": {},
   "source": [
    "```python\n",
    "# docling.py -> schemas/pdf_parser/models\n",
    "\n",
    "from src.schemas.pdf_parser.models import PaperFigure, PaperSection, PaperTable, ParserType, PdfContent\n",
    "\n",
    "\"\"\"\n",
    "class ParserType(str, Enum):\n",
    "    DOCLING = \"docling\"\n",
    "\n",
    "class PaperSection(BaseModel):\n",
    "    ...\n",
    "class PaperFigure(BaseModel):\n",
    "    ...\n",
    "class PaperTable(BaseModel):\n",
    "    ...\n",
    "class PdfContent(BaseModel):\n",
    "    ...\n",
    "class ArxivMetadata(BaseModel):\n",
    "    ...\n",
    "class ParsedPaper(BaseModel):\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "from .parser import PDFParserService\n",
    "\n",
    "class PDFParserService:\n",
    "    def __init__(self, max_pages: int, max_file_size_mb: int, do_ocr: bool = False, do_table_structure: bool = True):\n",
    "        self.docling_parser = DoclingParser(...)\n",
    "\n",
    "    async def parse_pdf(self, pdf_path: Path) -> Optional[PdfContent]:\n",
    "        \"\"\"Parse PDF using Docling parser only.\"\"\"\n",
    "        # result = await self.docling_parser.parse_pdf(pdf_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8761501",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f16541",
   "metadata": {},
   "source": [
    "```python\n",
    "# from src.schemas.pdf_parser.models import PaperFigure, PaperSection, PaperTable, ParserType, PdfContent\n",
    "import pypdfium2 as pdfium\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "class DoclingParser:\n",
    "    def __init__(self, max_pages: int, max_file_size_mb: int, do_ocr: bool = False, do_table_structure: bool = True):\n",
    "        # Configure pipeline options\n",
    "        pipeline_options = PdfPipelineOptions(...)\n",
    "        self._converter = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)})\n",
    "        self._warmed_up = False\n",
    "        self.max_pages = max_pages\n",
    "        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024\n",
    "\n",
    "    def _warm_up_models(self):\n",
    "        \"\"\"Pre-warm the models with a small dummy document to avoid cold start.\"\"\"\n",
    "        if not self._warmed_up:\n",
    "            # This happens only once per DoclingParser instance\n",
    "            self._warmed_up = True\n",
    "\n",
    "    def _validate_pdf(self, pdf_path: Path) -> bool:\n",
    "        \"\"\"Comprehensive PDF validation including size and page limits.\"\"\"\n",
    "        try:\n",
    "            # Check file exists and is not empty\n",
    "            file_size = pdf_path.stat().st_size\n",
    "            # if file_size > self.max_file_size_bytes: log warning,raiser error\n",
    "            # Check if file starts with PDF header\n",
    "            with open(pdf_path, \"rb\") as f:\n",
    "                header = f.read(8)\n",
    "                if not header.startswith(b\"%PDF-\"):\n",
    "                   # rasie error\n",
    "            # Check page count limit\n",
    "            pdf_doc = pdfium.PdfDocument(str(pdf_path))\n",
    "            actual_pages = len(pdf_doc)\n",
    "            pdf_doc.close()\n",
    "            # if actual_pages > self.max_pages: log warning,raiser error\n",
    "            return True\n",
    "\n",
    "        except (PDFValidationError,Exception) as e:\n",
    "            # raise proper error\n",
    "\n",
    "    async def parse_pdf(self, pdf_path: Path) -> Optional[PdfContent]:\n",
    "        \"\"\"Parse PDF using Docling parser.\n",
    "        Limited to 20 pages to avoid memory issues with large papers.\"\"\"\n",
    "        self._validate_pdf(pdf_path)\n",
    "        self._warm_up_models()\n",
    "        # Limit processing to avoid memory issues with large papers\n",
    "        result = self._converter.convert(str(pdf_path), max_num_pages=self.max_pages, max_file_size=self.max_file_size_bytes)\n",
    "        doc = result.document\n",
    "        sections = []\n",
    "        current_section = {\"title\": \"Content\", \"content\": \"\"}\n",
    "        for element in doc.texts:\n",
    "            if hasattr(element, \"label\") and element.label in [\"title\", \"section_header\"]:\n",
    "                # Save previous section if it has content\n",
    "                if current_section[\"content\"].strip():\n",
    "                    sections.append(PaperSection(title=current_section[\"title\"], content=current_section[\"content\"].strip()))\n",
    "                # Start new section\n",
    "                current_section = {\"title\": element.text.strip(), \"content\": \"\"}\n",
    "            else:\n",
    "                # Add content to current section\n",
    "                if hasattr(element, \"text\") and element.text:\n",
    "                    current_section[\"content\"] += element.text + \"\\n\"\n",
    "        # Add final section\n",
    "        if current_section[\"content\"].strip():\n",
    "            sections.append(PaperSection(title=current_section[\"title\"], content=current_section[\"content\"].strip()))\n",
    "\n",
    "        return PdfContent(\n",
    "            sections=sections,\n",
    "            figures=[],  # Removed: basic metadata not useful\n",
    "            tables=[],  # Removed: basic metadata not useful\n",
    "            raw_text=doc.export_to_text(),\n",
    "            references=[],\n",
    "            parser_used=ParserType.DOCLING,\n",
    "            metadata={\"source\": \"docling\", \"note\": \"Content extracted from PDF, metadata comes from arXiv API\"},\n",
    "        )\n",
    "    except PDFValidationError as error_msg:\n",
    "        # filter & display error_msg - \"too large\",\"too many pages\"\n",
    "    except Exception as error_msg:\n",
    "        # filter & dispaly error_msg - \"not valid\",\"timeout\",\"memory\",\"max_num_pages\",\"other\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
