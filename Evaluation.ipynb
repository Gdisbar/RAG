{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHu9HzZBow3K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word based (Statistical)**\n",
        "--------------------------------------------------\n",
        "**BLEU** - how many word seq in generated output matches ground truth.Used for translation & generation accuracy.Focus on Precision (correct n-gram output).\n",
        "\n",
        "**ROUGE**- overlap of n-gram between generated & reference summary.Focus on Recall (how much of ref. is captured).\n",
        "\n",
        "**METEOR** - harmonic mean of Precision & Recall to adjust word order difference btw original & expected output.Focus on model confidence in output likelihood.\n",
        "\n",
        "**Perplexity** - how well a model predicts next word in a sequence. Lower is better.Focus on model confidence in output likelihood."
      ],
      "metadata": {
        "id": "4v6IshimtgQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Character based (Statistical)**\n",
        "----------------------------------------------------\n",
        "**Edit distance** - min no of single-character edits to change one word to other.\n",
        "\n",
        "# **Embedding based(Statistical + LLM based)**\n",
        "-----------------------------------------------\n",
        "\n",
        "**BERTScore** - Semantic evaluation using contextual embedding rather than exact match.\n",
        "Measuring how well the reference’s tokens are covered by the candidate."
      ],
      "metadata": {
        "id": "th2iK1c6tlYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP models (LLM as judge)**\n",
        "---------------------------------------------------\n",
        "**NLI** - use NLP classification model to classify whether an LLM output is logically consistent (entailment), contradictory, or unrelated (neutral) with respect to a given reference text.\n",
        "The score typically ranges between entailment (with a value of 1) and contradiction (with a value of 0).\n",
        "\n",
        "Example dataset - *MultiNLI (Multi-Genre NLI): Expands SNLI to multiple genres and registers, supporting better generalization across domains.*\n",
        "\n",
        "**Answer relevance (pairwise comparison)** - used for user-facing application (chatbot).\n",
        "\n",
        "  Example dataset - *MS MARCO : widely used for training LLMs to judge answer relevance and rank candidate answers, Natural Questions: used for evaluating context relevance in retrieval-based chatbot*\n",
        "\n",
        "**Fluency & Coherence** (grammar, sentence flow, logical progress) - human-as-judge on Likert scale.\n",
        "\n",
        "Example dataset - *IT-ConvAI2 and Blended Skill Talk(uses Likert scales): Common for benchmarking generation coherence and grammar*\n",
        "\n",
        "**Toxicity** - Classification using curated dataset\n",
        "\n",
        "Example dataset -  *Jigsaw Toxic Comment* a multilingual dataset for online toxicity, offensive speech, and abuse identification\n",
        "\n"
      ],
      "metadata": {
        "id": "-Fl-9PuHo2D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Retrieval**\n",
        "---------------------------------------------------\n",
        "**Mean Reciprocal Rank** - avg of reciprocal ranks of the first relevant doc across queries.\n",
        "Rewards systems that surface relevant doc earlier.\n",
        "\n",
        "**Normalized Discounted Cumulative Gain (nDCG@k)** - scores ranked retrieval results by summing relevance scores weighted by discount factor that decrease with rank position.\n",
        "Capture both relevance grading & rank order.\n",
        "\n",
        "**Context-Recall** - Proportion of queries for which all necessary support passages are retrieved (regardless of order). Ensure the retrieval module brings back every piece of evidence needed.\n",
        "\n",
        "**Precision@k** - proportion of top-k retrieved documents that are relevant. Focuses on retrieval accuracy within the top set.\n",
        "\n",
        "**Recall@k** - proportion of all relevant documents that appear in top-k retrieved set. Measures completeness of retrieval."
      ],
      "metadata": {
        "id": "0nHfGs6gqx9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Major LLM Benchmarks and Their Purposes\n",
        "\n",
        "| Benchmark    | Main Use / Evaluation Area                           | Typical Scenario or Task                                  |\n",
        "|--------------|-----------------------------------------------------|----------------------------------------------------------|\n",
        "| MMLU         | General knowledge, reasoning, QA                    | Multiple-choice exams from 57 domains, e.g., history, law, STEM; measures breadth of model knowledge[1][5][2] |\n",
        "| HellaSwag    | Commonsense reasoning, contextual inference         | Completing sentences/stories with the most plausible ending after adversarial filtering; detect real-world logic gaps[1][4][5] |\n",
        "| BIG-bench    | Advanced reasoning, multi-step tasks, creativity    | Diverse set of tasks, including cognitive tests, logical puzzles, error analysis and complex linguistic challenges[1][5] |\n",
        "| DROP         | Discrete reasoning, reading comprehension           | Requires sorting, counting, extraction and reasoning over long paragraphs (from Wikipedia)[1][4] |\n",
        "| SQuAD        | Extractive question answering                       | Answering factual questions from Wikipedia passages; evaluates span extraction ability[1][5] |\n",
        "| TruthfulQA   | Truthfulness in generated answers                   | Tests for avoidance of hallucinations and generation of factually correct statements[4] |\n",
        "| GSM-8K       | Grade-school mathematical reasoning                 | Word problems requiring arithmetic/logical computation, tests chain-of-thought abilities[4][5] |\n",
        "| MATH         | Advanced mathematics (problem solving, reasoning)   | University-level math challenges, algebra, calculus, competition problems[5] |\n",
        "| HumanEval    | Code generation correctness                         | Writing functions in Python from docstrings, focuses on code accuracy and execution[4][2][5] |\n",
        "| MBPP         | Basic Python programming, functional correctness    | 900+ code generation tasks, measures performance on simple algorithmic problems[2] |\n",
        "| MT-Bench     | Dialog/chat capabilities, instruction following     | Multi-turn conversation tests; covers coding, extraction, knowledge, roleplay, and more[2][5] |\n",
        "| GLUE         | Core NLP tasks (classification, sentiment, QA)      | Suite of nine text tasks, yields aggregate language understanding score; widely used for baseline evaluations[5] |\n",
        "| Chatbot Arena| Human preference, dialog/response quality           | Human crowdworkers compare conversational responses; measures preference and engagement[4][6] |\n",
        "| LegalBench   | Legal text processing (domain-specific)             | Assessing if a model can interpret statutes, case law, legal arguments[3] |\n",
        "| MedQA        | Medical knowledge, clinical reasoning               | Exam-style questions for doctors, patient scenario analysis; healthcare domain safety[3] |\n",
        "\n",
        "### Matching Benchmarks to Use Cases\n",
        "\n",
        "- **General-Purpose Evaluation**: MMLU, BIG-bench, GLUE — test versatility in broad applications and basic NLP.[5][1]\n",
        "- **Knowledge & Reasoning**: MMLU, DROP, SQuAD, TruthfulQA — chosen for fact-extraction, comprehension, knowledge QA bots.[4][1]\n",
        "- **Commonsense & Logic**: HellaSwag, BIG-bench, DROP — detect hallucinations, test model’s real-world reasoning.[1][4]\n",
        "- **Math/Chain-of-Thought**: GSM-8K, MATH — important for agents needing stepwise math or logical planning.[4][5]\n",
        "- **Coding**: HumanEval, MBPP — development assistants, IDE integrations, code-generation APIs.[2][4]\n",
        "- **Chat, Conversation**: MT-Bench, Chatbot Arena — dialog agents, customer support bots, personalization.[6][2]\n",
        "- **Domain-Specific**: LegalBench, MedQA, FinanceBench — specialty applications in law, medicine, finance.[3]\n",
        "\n",
        "Every benchmark provides reproducible ways to probe whether an LLM is fit for general production rollout or requires targeted improvement via fine-tuning or transfer learning. For new model deployments, matching the benchmark’s domain and task structure to the application ensures robust pre-launch quality control.[3][5][1][4]\n",
        "\n"
      ],
      "metadata": {
        "id": "bYPS8ljgtJ1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some famous datasets used for evaluating different types of LLMs and Retrieval-Augmented Generation (RAG) systems, categorized by their core purpose:\n",
        "\n",
        "### Datasets for General LLM Evaluation\n",
        "\n",
        "- **MMLU (Massive Multitask Language Understanding)**: Multitask benchmark covering diverse domains to evaluate general knowledge and reasoning.\n",
        "- **HellaSwag**: Commonsense reasoning and contextual inference.\n",
        "- **BIG-bench**: Wide-ranging tasks including reasoning, creativity, and linguistic knowledge.\n",
        "- **OpenWebText**: Large-scale web text for training and evaluation of language understanding.\n",
        "- **Stanford Human Preferences (SHP)**: Human preference annotations useful for RLHF and naturalness evaluation.\n",
        "\n",
        "### Datasets for Code Generation and Reasoning\n",
        "\n",
        "- **HumanEval**: Python function generation from docstring prompts.\n",
        "- **MBPP (Mostly Basic Python Problems)**: Basic coding challenge set.\n",
        "- **OpenMathInstruct-1**: Math problem solutions combining text and Python code.\n",
        "\n",
        "### Datasets for RAG Evaluation\n",
        "\n",
        "- **HotpotQA**: Multi-hop question answering requiring retrieving multiple documents.\n",
        "- **MS MARCO**: Large-scale passage ranking for retrieval evaluation.\n",
        "- **Natural Questions**: Real user queries with Wikipedia evidence passages.\n",
        "- **CovidQA, PubmedQA**: Biomedical and clinical question answering datasets.\n",
        "- **CuAD**: Legal contracts QA dataset.\n",
        "- **FinQA, TAT-QA**: Financial QA datasets requiring numerical and tabular reasoning.\n",
        "- **FRAMES (Factuality, Retrieval, And reasoning MEasurement Set)**: Tests retrieval accuracy, reasoning, and factuality for end-to-end RAG systems.\n",
        "- **RAGTruth**: Designed to analyze hallucination and assess hallucination detection techniques in RAG outputs.\n",
        "- **DragonBall Dataset**: Multilingual, multi-domain RAG benchmark with diverse QA pairs.\n",
        "\n",
        "### Additional Specialized LLM Datasets\n",
        "\n",
        "- **OpenOrca**: Reasoning-focused dataset combining large-scale GPT-4/3.5 completions.\n",
        "- **Stanford HellaSwag**: Dataset for commonsense reasoning.\n",
        "- **Ai2 ARC (AI2 Reasoning Challenge)**: Science questions requiring reasoning.\n",
        "- **TriviaQA**: Open-domain question answering requiring external knowledge.\n",
        "\n"
      ],
      "metadata": {
        "id": "cd9Ifb81tWk2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfxEgZcixJPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}